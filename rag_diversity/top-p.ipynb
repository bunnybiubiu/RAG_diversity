{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d09b68f-6909-4631-af4c-972f6b6ffd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from evaluation import EvaluationMetrics\n",
    "import time\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3886dc65-7a6b-4b21-a424-3a82192c0cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom scipy.spatial.distance import cosine\\n\\nclass CustomWikipediaRetriever:\\n    def __init__(self, k=3, num_doc=20, top_p=1.0, temperature=1.0, std_error=0.0, embedding_model=None):\\n        self.retriever = WikipediaRetriever(top_k_results=num_doc)  # Now retrieves 50 documents\\n        self.embedding_model = embedding_model or OpenAIEmbeddings()\\n        self.k = k  # Number of documents to sample\\n        self.temperature = temperature  # Temperature scaling for softmax\\n        self.std_error = std_error\\n        self.top_p = top_p  # top-p probability mass to consider\\n\\n    def softmax(self, similarities, temperature):\\n        \"\"\"Apply softmax with temperature to convert similarities into probabilities.\"\"\"\\n        e_similarities = np.exp(np.array(similarities) / temperature)\\n        return e_similarities / np.sum(e_similarities)\\n\\n    def add_noise_to_probabilities(self, probabilities):\\n        \"\"\"Add normal distributed noise to the probabilities.\"\"\"\\n        noise = np.random.normal(0, self.std_error, len(probabilities))\\n        noisy_probabilities = probabilities + noise\\n        \\n        # Ensure all probabilities are non-negative\\n        noisy_probabilities = np.maximum(noisy_probabilities, 0)\\n        \\n        # Normalize the probabilities so they sum to 1\\n        return noisy_probabilities / np.sum(noisy_probabilities)\\n\\n    def retrieve_with_similarity(self, query):\\n        # Get top-50 documents from the retriever\\n        docs = self.retriever.get_relevant_documents(query)\\n        \\n        # Calculate the similarity scores\\n        query_embedding = self.embedding_model.embed_query(query)\\n        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\\n        \\n        # Calculate cosine similarity scores\\n        similarities = [1 - cosine(query_embedding, doc_emb) for doc_emb in doc_embeddings]\\n        \\n        # Convert similarities to probabilities\\n        probabilities = self.softmax(similarities, self.temperature)\\n\\n        # Add noise if needed\\n        noisy_probabilities = self.add_noise_to_probabilities(probabilities)\\n\\n        # Sort documents by their noisy probabilities (in descending order)\\n        sorted_indices = np.argsort(noisy_probabilities)[::-1]\\n        sorted_docs = [docs[i] for i in sorted_indices]\\n        sorted_probabilities = noisy_probabilities[sorted_indices]\\n\\n        # Accumulate probabilities and select documents for top-p\\n        cumulative_probabilities = np.cumsum(sorted_probabilities)\\n        selected_docs = []\\n        \\n        for i, prob in enumerate(cumulative_probabilities):\\n            if prob <= self.top_p:\\n                selected_docs.append(sorted_docs[i])\\n            else:\\n                break\\n\\n        # Normalize the probabilities of the selected docs to sum to 1\\n        selected_probabilities = sorted_probabilities[:len(selected_docs)]\\n        normalized_probabilities = selected_probabilities / np.sum(selected_probabilities)\\n\\n        # Randomly sample k documents from the selected top-p documents\\n        sampled_indices = np.random.choice(len(selected_docs), size=self.k, replace=False, p=normalized_probabilities)\\n        results = [selected_docs[i] for i in sampled_indices]\\n\\n        return results\\n\\n\\nclass QAChain:\\n    def __init__(self, k: int = 3, num_doc=20, top_p: float = 1.0, temperature=1.0, std_error=0.0, embedding_model=None):\\n        \"\"\"\\n        Initialize the QAChain class with the desired number of retrieved documents (k).\\n\\n        Parameters:\\n        - k: Number of documents to retrieve from Wikipedia for context (default is 3).\\n        - embedding_model: The embedding model to use (default is OpenAIEmbeddings).\\n        \"\"\"\\n        self.retriever = CustomWikipediaRetriever(k=k, num_doc=num_doc, top_p=top_p, temperature=temperature, std_error=std_error, embedding_model=embedding_model)\\n\\n        # Define the prompt template\\n        self.prompt = ChatPromptTemplate.from_template(\\n            \"\"\"Answer the question based only on the context provided as short as possible.\\n\\n            Context: {context}\\n\\n            Question: {question}\"\"\"\\n        )\\n        \\n        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\\n\\n        # Create the full chain\\n        self.chain = (\\n            {\"context\": self.retrieve_docs,  # Fixed method reference\\n             \"question\": RunnablePassthrough()}\\n            | self.prompt\\n            | self.llm\\n            | StrOutputParser()\\n        )\\n\\n    def retrieve_docs(self, query):\\n        \"\"\"\\n        Retrieve and format documents from Wikipedia for the chain.\\n        \"\"\"\\n        docs = self.retriever.retrieve_with_similarity(query)\\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\\n    \\n    def answer(self, question: str):\\n        \"\"\"\\n        Answer a given question based on the context retrieved from Wikipedia.\\n\\n        Parameters:\\n        - question: The question to ask.\\n        \"\"\"\\n        return self.chain.invoke(question)\\n\\n\\n# Instantiate the QAChain without explicitly passing an embedding model\\nqa_chain = QAChain(top_p=0.9)  # You can adjust top_p as per your needs\\n\\n# Retrieve documents with similarity scores and provide the answer\\nquery = \"What is the capital of France?\"\\nanswer = qa_chain.answer(query)\\n\\n# Print the answer\\nprint(answer)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This part is for normal RAG pipeline\n",
    "'''\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "class CustomWikipediaRetriever:\n",
    "    def __init__(self, k=3, num_doc=20, top_p=1.0, temperature=1.0, std_error=0.0, embedding_model=None):\n",
    "        self.retriever = WikipediaRetriever(top_k_results=num_doc)  # Now retrieves 50 documents\n",
    "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
    "        self.k = k  # Number of documents to sample\n",
    "        self.temperature = temperature  # Temperature scaling for softmax\n",
    "        self.std_error = std_error\n",
    "        self.top_p = top_p  # top-p probability mass to consider\n",
    "\n",
    "    def softmax(self, similarities, temperature):\n",
    "        \"\"\"Apply softmax with temperature to convert similarities into probabilities.\"\"\"\n",
    "        e_similarities = np.exp(np.array(similarities) / temperature)\n",
    "        return e_similarities / np.sum(e_similarities)\n",
    "\n",
    "    def add_noise_to_probabilities(self, probabilities):\n",
    "        \"\"\"Add normal distributed noise to the probabilities.\"\"\"\n",
    "        noise = np.random.normal(0, self.std_error, len(probabilities))\n",
    "        noisy_probabilities = probabilities + noise\n",
    "        \n",
    "        # Ensure all probabilities are non-negative\n",
    "        noisy_probabilities = np.maximum(noisy_probabilities, 0)\n",
    "        \n",
    "        # Normalize the probabilities so they sum to 1\n",
    "        return noisy_probabilities / np.sum(noisy_probabilities)\n",
    "\n",
    "    def retrieve_with_similarity(self, query):\n",
    "        # Get top-50 documents from the retriever\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # Calculate the similarity scores\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\n",
    "        \n",
    "        # Calculate cosine similarity scores\n",
    "        similarities = [1 - cosine(query_embedding, doc_emb) for doc_emb in doc_embeddings]\n",
    "        \n",
    "        # Convert similarities to probabilities\n",
    "        probabilities = self.softmax(similarities, self.temperature)\n",
    "\n",
    "        # Add noise if needed\n",
    "        noisy_probabilities = self.add_noise_to_probabilities(probabilities)\n",
    "\n",
    "        # Sort documents by their noisy probabilities (in descending order)\n",
    "        sorted_indices = np.argsort(noisy_probabilities)[::-1]\n",
    "        sorted_docs = [docs[i] for i in sorted_indices]\n",
    "        sorted_probabilities = noisy_probabilities[sorted_indices]\n",
    "\n",
    "        # Accumulate probabilities and select documents for top-p\n",
    "        cumulative_probabilities = np.cumsum(sorted_probabilities)\n",
    "        selected_docs = []\n",
    "        \n",
    "        for i, prob in enumerate(cumulative_probabilities):\n",
    "            if prob <= self.top_p:\n",
    "                selected_docs.append(sorted_docs[i])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Normalize the probabilities of the selected docs to sum to 1\n",
    "        selected_probabilities = sorted_probabilities[:len(selected_docs)]\n",
    "        normalized_probabilities = selected_probabilities / np.sum(selected_probabilities)\n",
    "\n",
    "        # Randomly sample k documents from the selected top-p documents\n",
    "        sampled_indices = np.random.choice(len(selected_docs), size=self.k, replace=False, p=normalized_probabilities)\n",
    "        results = [selected_docs[i] for i in sampled_indices]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, k: int = 3, num_doc=20, top_p: float = 1.0, temperature=1.0, std_error=0.0, embedding_model=None):\n",
    "        \"\"\"\n",
    "        Initialize the QAChain class with the desired number of retrieved documents (k).\n",
    "\n",
    "        Parameters:\n",
    "        - k: Number of documents to retrieve from Wikipedia for context (default is 3).\n",
    "        - embedding_model: The embedding model to use (default is OpenAIEmbeddings).\n",
    "        \"\"\"\n",
    "        self.retriever = CustomWikipediaRetriever(k=k, num_doc=num_doc, top_p=top_p, temperature=temperature, std_error=std_error, embedding_model=embedding_model)\n",
    "\n",
    "        # Define the prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Answer the question based only on the context provided as short as possible.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "        # Create the full chain\n",
    "        self.chain = (\n",
    "            {\"context\": self.retrieve_docs,  # Fixed method reference\n",
    "             \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def retrieve_docs(self, query):\n",
    "        \"\"\"\n",
    "        Retrieve and format documents from Wikipedia for the chain.\n",
    "        \"\"\"\n",
    "        docs = self.retriever.retrieve_with_similarity(query)\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    def answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Answer a given question based on the context retrieved from Wikipedia.\n",
    "\n",
    "        Parameters:\n",
    "        - question: The question to ask.\n",
    "        \"\"\"\n",
    "        return self.chain.invoke(question)\n",
    "\n",
    "\n",
    "# Instantiate the QAChain without explicitly passing an embedding model\n",
    "qa_chain = QAChain(top_p=0.9)  # You can adjust top_p as per your needs\n",
    "\n",
    "# Retrieve documents with similarity scores and provide the answer\n",
    "query = \"What is the capital of France?\"\n",
    "answer = qa_chain.answer(query)\n",
    "\n",
    "# Print the answer\n",
    "print(answer)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca6d01a5-af59-49f1-87dd-0f0128a6013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151833/2277842990.py:34: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = self.retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This list of Russian physicists includes the famous physicists from the Russian Empire, the Soviet U\n",
      "There is not a single founder of quantum physics, as it was developed by multiple physicists including Niels Bohr, Max Planck, Werner Heisenberg, and Erwin Schrödinger, among others.\n",
      "This list of Russian physicists includes the famous physicists from the Russian Empire, the Soviet U\n",
      "There is no clear mention of a single founder of quantum physics in the provided context.\n",
      "Quantum mysticism, sometimes referred to pejoratively as quantum quackery or quantum woo, is a set o\n",
      "The founders of quantum physics are Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born, Paul Dirac, and others.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "class CustomWikipediaRetriever:\n",
    "    def __init__(self, max_m=20, embedding_model=None):\n",
    "        self.retriever = WikipediaRetriever(top_k_results=max_m)  # Now retrieves 50 documents\n",
    "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
    "        self.docs = []\n",
    "        self.similarities = []\n",
    "        self.probabilities = []\n",
    "        self.selected_docs = []\n",
    "        self.selected_similarities = []\n",
    "        self.top_p_probabilities = []\n",
    "\n",
    "    def softmax(self, temperature, similarities):\n",
    "        \"\"\"Apply softmax with temperature to convert similarities into probabilities.\"\"\"\n",
    "        e_similarities = np.exp(np.array(similarities) / temperature)\n",
    "        return e_similarities / np.sum(e_similarities)\n",
    "\n",
    "    def add_noise_to_probabilities(self, probabilities, std_error):\n",
    "        \"\"\"Add normal distributed noise to the probabilities.\"\"\"\n",
    "        noise = np.random.normal(0, std_error, len(probabilities))\n",
    "        noisy_probabilities = probabilities + noise\n",
    "        \n",
    "        # Ensure all probabilities are non-negative\n",
    "        noisy_probabilities = np.maximum(noisy_probabilities, 0)\n",
    "        \n",
    "        # Normalize the probabilities so they sum to 1\n",
    "        return noisy_probabilities / np.sum(noisy_probabilities)\n",
    "\n",
    "    def retrieve_with_similarity(self, query):\n",
    "        # Get top-50 documents from the retriever\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # Calculate the similarity scores\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\n",
    "        \n",
    "        # Calculate cosine similarity scores\n",
    "        similarities = [1 - cosine(query_embedding, doc_emb) for doc_emb in doc_embeddings]\n",
    "\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        sorted_docs = [docs[i] for i in sorted_indices]\n",
    "        sorted_similarities = np.array(similarities)[sorted_indices]\n",
    "        probabilities = self.softmax(1.0, sorted_similarities)\n",
    "         \n",
    "        self.docs = sorted_docs\n",
    "        self.similarities = sorted_similarities\n",
    "        self.probabilities = probabilities\n",
    "\n",
    "\n",
    "    def set_top_p(self, top_p=0.9):\n",
    "\n",
    "        # Accumulate probabilities and select documents for top-p\n",
    "        cumulative_probabilities = np.cumsum(self.probabilities)\n",
    "\n",
    "        i=0\n",
    "        while i<len(cumulative_probabilities):\n",
    "            if cumulative_probabilities[i]>top_p:\n",
    "                break\n",
    "            i += 1\n",
    "        \n",
    "        self.selected_docs = self.docs[:i]\n",
    "        self.selected_similarities = self.similarities[:i]\n",
    "        \n",
    "\n",
    "    def retrieve(self, k=3, temperature=1.0, std_error=0.0):\n",
    "    \n",
    "        probabilities = self.softmax(temperature, self.selected_similarities)\n",
    "        \n",
    "        # Add noise if needed\n",
    "        noisy_probabilities = self.add_noise_to_probabilities(probabilities, std_error)\n",
    "        \n",
    "        # Randomly sample k documents from the selected top-p documents\n",
    "        sampled_indices = np.random.choice(len(self.selected_docs), size=min(len(self.selected_docs),k), replace=False, p=noisy_probabilities)\n",
    "        results = [self.selected_docs[i] for i in sampled_indices]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, docs, embedding_model=None):\n",
    "        \"\"\"\n",
    "        Initialize the QAChain class with the desired number of retrieved documents (k).\n",
    "\n",
    "        Parameters:\n",
    "        - k: Number of documents to retrieve from Wikipedia for context (default is 3).\n",
    "        - embedding_model: The embedding model to use (default is OpenAIEmbeddings).\n",
    "        \"\"\"\n",
    "        self.docs = docs\n",
    "\n",
    "        # Define the prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Answer the question based only on the context provided as short as possible.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "        # Create the full chain\n",
    "        self.chain = (\n",
    "            {\"context\": self.retrieve_docs,  # Fixed method reference\n",
    "             \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def retrieve_docs(self, query):\n",
    "        \"\"\"\n",
    "        Retrieve and format documents from Wikipedia for the chain.\n",
    "        \"\"\"\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in self.docs)\n",
    "    \n",
    "    def answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Answer a given question based on the context retrieved from Wikipedia.\n",
    "\n",
    "        Parameters:\n",
    "        - question: The question to ask.\n",
    "        \"\"\"\n",
    "        return self.chain.invoke(question)\n",
    "\n",
    "\n",
    "\n",
    "retriever = CustomWikipediaRetriever()\n",
    "query = \"who is the founder of quantum physics?\"\n",
    "retriever.retrieve_with_similarity(query)\n",
    "retriever.set_top_p()\n",
    "\n",
    "for i in range(3):\n",
    "    docs = retriever.retrieve()\n",
    "    print(docs[0].page_content[:100])\n",
    "    qa_chain = QAChain(docs=docs)\n",
    "    answer = qa_chain.answer(query)\n",
    "    print(answer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f70af95-b5af-46d7-9edd-d32ca901dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = [0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95]\n",
    "k_test = [1, 2, 3, 4, 5, 6]\n",
    "t_test = [0.1, 0.5, 1.0, 2.0, 10.0]\n",
    "e_test = [0.05, 0.1, 0.15, 0.2]\n",
    "num_sample = 500\n",
    "num_times = 3\n",
    "rng = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00fc96d7-1dbb-4e39-8429-5a12d726dc2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eval = EvaluationMetrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0157c77f-0119-4345-9496-3592098cc075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataset import get_nq, get_tqa, get_squad, get_asqa\n",
    "\n",
    "nq = get_nq()\n",
    "tqa = get_tqa()\n",
    "squad = get_squad()\n",
    "asqa = get_asqa()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56649961-e342-4542-9da9-2a0fafca4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_ds = {\"NQ\": nq, \"TriviaQA\": tqa, \"SQuAD\": squad, \"ASQA\": asqa}\n",
    "\n",
    "def evaluate(name):\n",
    "    ds = name_to_ds[name]\n",
    "    os.makedirs(\"top-p_results/vary_p\", exist_ok=True)\n",
    "    os.makedirs(\"top-p_results/vary_k\", exist_ok=True)\n",
    "    os.makedirs(\"top-p_results/temperature\", exist_ok=True)\n",
    "    os.makedirs(\"top-p_results/std_error\", exist_ok=True)\n",
    "\n",
    "    \n",
    "    random.seed(rng)\n",
    "    selected_idx = random.sample(range(ds.shape[0]), num_sample)\n",
    "\n",
    "    references = ds.loc[selected_idx, \"answer\"].values.tolist()\n",
    "    \n",
    "    p_candidates = [[] for _ in range(len(p_test))]\n",
    "    p_results = []\n",
    "    k_candidates = [[] for _ in range(len(k_test))]\n",
    "    k_results = []\n",
    "    t_candidates = [[] for _ in range(len(t_test))]\n",
    "    t_results = []\n",
    "    e_candidates = [[] for _ in range(len(e_test))]\n",
    "    e_results = []\n",
    "    \n",
    "    for i in selected_idx:\n",
    "        q = ds.loc[i, \"question\"]\n",
    "        a = ds.loc[i, \"answer\"]\n",
    "        retriever = CustomWikipediaRetriever()\n",
    "        retriever.retrieve_with_similarity(q)\n",
    "        \n",
    "        for j in range(len(p_test)):\n",
    "            top_p = p_test[j]\n",
    "            retriever.set_top_p(top_p=top_p)\n",
    "            c = []\n",
    "            for _ in range(num_times):\n",
    "                docs = retriever.retrieve()\n",
    "                qa_chain = QAChain(docs=docs)\n",
    "                answer = qa_chain.answer(q)\n",
    "                c.append(answer)\n",
    "                time.sleep(0.03)\n",
    "                p_results.append({\"idx in original dataset\":i, \"question\":q , \"top-p\": top_p, \"num_times\":_, \"retrieved docs\": docs, \"generated answer\": answer, \"standard answer\": a})\n",
    "            p_candidates[j].append(c)\n",
    "\n",
    "        retriever.set_top_p()\n",
    "        \n",
    "        for j in range(len(k_test)):\n",
    "            k = k_test[j]\n",
    "            c = []\n",
    "            for _ in range(num_times):\n",
    "                docs = retriever.retrieve(k=k)\n",
    "                qa_chain = QAChain(docs=docs)\n",
    "                answer = qa_chain.answer(q)\n",
    "                c.append(answer)\n",
    "                time.sleep(0.03)\n",
    "                k_results.append({\"idx in original dataset\":i, \"question\":q , \"k\": k, \"num_times\":_, \"retrieved docs\": docs, \"generated answer\": answer, \"standard answer\": a})\n",
    "            k_candidates[j].append(c)\n",
    "            \n",
    "        for j in range(len(t_test)):\n",
    "            t = t_test[j]\n",
    "            c = []\n",
    "            for _ in range(num_times):\n",
    "                docs = retriever.retrieve(temperature=t)\n",
    "                qa_chain = QAChain(docs=docs)\n",
    "                answer = qa_chain.answer(q)\n",
    "                c.append(answer)\n",
    "                time.sleep(0.03)\n",
    "                t_results.append({\"idx in original dataset\":i, \"question\":q , \"temperature\": t, \"num_times\":_, \"retrieved docs\": docs, \"generated answer\": answer, \"standard answer\": a})\n",
    "            t_candidates[j].append(c)\n",
    "            \n",
    "        for j in range(len(e_test)):\n",
    "            e = e_test[j]\n",
    "            c = []\n",
    "            for _ in range(num_times):\n",
    "                docs = retriever.retrieve(std_error=e)\n",
    "                qa_chain = QAChain(docs=docs)\n",
    "                answer = qa_chain.answer(q)\n",
    "                c.append(answer)\n",
    "                time.sleep(0.03)\n",
    "                e_results.append({\"idx in original dataset\":i, \"question\":q , \"standard error\": e, \"num_times\":_, \"retrieved docs\": docs, \"generated answer\": answer, \"standard answer\": a})\n",
    "            e_candidates[j].append(c)\n",
    "\n",
    "    p_results = pd.DataFrame(p_results)\n",
    "    p_results.to_csv(f'top-p_results/vary_p/{name}.csv')\n",
    "    \n",
    "    k_results = pd.DataFrame(k_results)\n",
    "    k_results.to_csv(f'top-p_results/vary_k/{name}.csv')\n",
    "    \n",
    "    t_results = pd.DataFrame(t_results)\n",
    "    t_results.to_csv(f'top-p_results/temperature/{name}.csv')\n",
    "    \n",
    "    e_results = pd.DataFrame(e_results)\n",
    "    e_results.to_csv(f'top-p_results/std_error/{name}.csv')\n",
    "    \n",
    "    \n",
    "    p_rougeL, p_diversity = eval.cal_scores(p_candidates, references)\n",
    "    with open(f'top-p_results/vary_p/{name}.txt', 'w') as file:\n",
    "        sys.stdout = file\n",
    "        print(\"The top-p docs p are:\", p_test)\n",
    "        print(\"rougeL score for different p is:\", p_rougeL)\n",
    "        print(\"diversity score for different p is:\", p_diversity)\n",
    "    sys.stdout = sys.__stdout__\n",
    "    \n",
    "    k_rougeL, k_diversity = eval.cal_scores(k_candidates, references)\n",
    "    with open(f'top-p_results/vary_k/{name}.txt', 'w') as file:\n",
    "        sys.stdout = file\n",
    "        print(\"The sampled k docs k are:\", k_test)\n",
    "        print(\"rougeL score for different k is:\", k_rougeL)\n",
    "        print(\"diversity score for different k is:\", k_diversity)\n",
    "    sys.stdout = sys.__stdout__\n",
    "    \n",
    "    t_rougeL, t_diversity = eval.cal_scores(t_candidates, references)\n",
    "    with open(f'top-p_results/temperature/{name}.txt', 'w') as file:\n",
    "        sys.stdout = file\n",
    "        print(\"The temperature t are:\", t_test)\n",
    "        print(\"rougeL score for different t is:\", t_rougeL)\n",
    "        print(\"diversity score for different t is:\", t_diversity)\n",
    "    sys.stdout = sys.__stdout__\n",
    "    \n",
    "    e_rougeL, e_diversity = eval.cal_scores(e_candidates, references)\n",
    "    with open(f'top-p_results/std_error/{name}.txt', 'w') as file:\n",
    "        sys.stdout = file\n",
    "        print(\"The standard error e are:\", e_test)\n",
    "        print(\"rougeL score for different e is:\", e_rougeL)\n",
    "        print(\"diversity score for different e is:\", e_diversity)\n",
    "    sys.stdout = sys.__stdout__\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce76371-0061-4ec8-add1-0e587267a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"NQ\", \"TriviaQA\", \"SQuAD\", \"ASQA\"]\n",
    "\n",
    "for ds in datasets:\n",
    "    evaluate(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989854c9-d438-4f71-aa35-980474929afa",
   "metadata": {},
   "source": [
    "Top-k method works badly when the relevance distribution of the corpus is extreme. For example, if only few documents are relevant with query, then top-k retrieval will return many irrelevant documents, leading to distraction. On the other hand, if many documents are relevant with query in different aspects, then top-k retrieval can't return all necessary documents.\n",
    "\n",
    "Top-p method solves this problem by applying a relative parameter p. The cumulative probability p is calculated based on all documents. It ensures how much relevant information will be returned. However, top-k method still has drawback, i.e., it can't solve the problem of repeatness in retrieved documents.\n",
    "\n",
    "The parameter num_doc is set to be 20 here just for quick demonstration. \n",
    "\n",
    "Actually I think 50 should be more reasonable. Because top-p method comes from its application in LLM. In LLM, the whole word population are used to calculate the normalized probabilities with softmax function. However, in retriever, since Wikipedia corpus is too large to calculate the similarity of the whole population. A number of num_doc of the most relevant documents are returned to approximate the whole population. num_doc=50 can guarantee that almost all relevant documents are returned by the retrieval. On the contrary, num_doc=20 can't guarantee this. That's why I think num_doc=50 is more reasonable for approximation of the whole population."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
