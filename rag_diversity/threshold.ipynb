{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6af1e42-fe3d-40fa-b118-6d7504c58ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from evaluation import EvaluationMetrics\n",
    "import sys\n",
    "import time\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665884ab-291a-4e72-a681-926d4ad05555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom scipy.spatial.distance import cosine\\nimport random\\n\\nclass CustomWikipediaRetriever:\\n    def __init__(self, k=3, top_m=10, embedding_model=None, threshold=0.7):\\n        self.retriever = WikipediaRetriever(top_k_results=top_m)\\n        self.embedding_model = embedding_model or OpenAIEmbeddings()\\n        self.k = k # Number of documents to sample \\n        self.docs = None\\n        self.doc_embeddings = None\\n        self.threshold = threshold\\n\\n    def retrieve(self, query):\\n        docs = self.retriever.get_relevant_documents(query)\\n        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\\n        doc_embeddings = [emb / np.linalg.norm(emb) for emb in doc_embeddings]\\n        \\n        selected_idx = [random.choice(range(len(docs)))]\\n        selected_embeddings = [doc_embeddings[selected_idx[0]]]\\n\\n        remaining_indices = list(set(range(len(docs))) - {selected_idx[0]})\\n        while len(selected_idx) < self.k and remaining_indices:\\n            candidate_idx = random.choice(remaining_indices)\\n            candidate_embedding = doc_embeddings[candidate_idx]\\n\\n            similarity_ok = all((1+cosine(candidate_embedding, emb))/2.0 < self.threshold for emb in selected_embeddings)\\n\\n            if similarity_ok:\\n                selected_idx.append(candidate_idx)\\n                selected_embeddings.append(candidate_embedding)\\n\\n            remaining_indices.remove(candidate_idx)\\n\\n        selected_docs = [docs[idx] for idx in selected_idx]\\n        return selected_docs\\n\\n\\nclass QAChain:\\n    def __init__(self, k: int = 3, top_m: int = 10, threshold: float = 0.7, embedding_model=None):\\n        self.retriever = CustomWikipediaRetriever(top_m=top_m, k=k, threshold=threshold, embedding_model=embedding_model)\\n        self.prompt = ChatPromptTemplate.from_template(\\n            \"\"\"Answer the question based only on the context provided as short as possible.\\n\\n            Context: {context}\\n\\n            Question: {question}\"\"\"\\n        )\\n        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\\n        \\n        self.chain = (\\n            {\"context\": self.retrieve_docs, \"question\": RunnablePassthrough()}\\n            | self.prompt\\n            | self.llm\\n            | StrOutputParser()\\n        ) \\n\\n    def retrieve_docs(self, query):\\n        \\n        # for normal RAG pipeline\\n        docs = self.retriever.retrieve(query)\\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\\n    \\n    \\n    def answer(self, question: str):\\n        return self.chain.invoke(question)\\n\\nqa_chain = QAChain(k=3)\\nquery = \"What is the capital of France?\"\\nanswer = qa_chain.answer(query)\\nprint(answer)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This part is for normal RAG pipeline\n",
    "\n",
    "'''\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "\n",
    "class CustomWikipediaRetriever:\n",
    "    def __init__(self, k=3, top_m=10, embedding_model=None, threshold=0.7):\n",
    "        self.retriever = WikipediaRetriever(top_k_results=top_m)\n",
    "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
    "        self.k = k # Number of documents to sample \n",
    "        self.docs = None\n",
    "        self.doc_embeddings = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\n",
    "        doc_embeddings = [emb / np.linalg.norm(emb) for emb in doc_embeddings]\n",
    "        \n",
    "        selected_idx = [random.choice(range(len(docs)))]\n",
    "        selected_embeddings = [doc_embeddings[selected_idx[0]]]\n",
    "\n",
    "        remaining_indices = list(set(range(len(docs))) - {selected_idx[0]})\n",
    "        while len(selected_idx) < self.k and remaining_indices:\n",
    "            candidate_idx = random.choice(remaining_indices)\n",
    "            candidate_embedding = doc_embeddings[candidate_idx]\n",
    "\n",
    "            similarity_ok = all((1+cosine(candidate_embedding, emb))/2.0 < self.threshold for emb in selected_embeddings)\n",
    "\n",
    "            if similarity_ok:\n",
    "                selected_idx.append(candidate_idx)\n",
    "                selected_embeddings.append(candidate_embedding)\n",
    "\n",
    "            remaining_indices.remove(candidate_idx)\n",
    "\n",
    "        selected_docs = [docs[idx] for idx in selected_idx]\n",
    "        return selected_docs\n",
    "\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, k: int = 3, top_m: int = 10, threshold: float = 0.7, embedding_model=None):\n",
    "        self.retriever = CustomWikipediaRetriever(top_m=top_m, k=k, threshold=threshold, embedding_model=embedding_model)\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Answer the question based only on the context provided as short as possible.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        \n",
    "        self.chain = (\n",
    "            {\"context\": self.retrieve_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        ) \n",
    "\n",
    "    def retrieve_docs(self, query):\n",
    "        \n",
    "        # for normal RAG pipeline\n",
    "        docs = self.retriever.retrieve(query)\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    \n",
    "    def answer(self, question: str):\n",
    "        return self.chain.invoke(question)\n",
    "\n",
    "qa_chain = QAChain(k=3)\n",
    "query = \"What is the capital of France?\"\n",
    "answer = qa_chain.answer(query)\n",
    "print(answer)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0116d09-3c5c-4e10-a160-eed1f209c061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nqa_chain.answer(\"who is the founder of quantum physics\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For normal RAG pipeline\n",
    "'''\n",
    "\n",
    "qa_chain.answer(\"who is the founder of quantum physics\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e629b60-ac5b-4cd7-9a48-badc08f29d2f",
   "metadata": {},
   "source": [
    "Using similarity threshold as parameter:\n",
    "similarity_ok = all((1+cosine(candidate_embedding, emb))/2.0 < self.similarity_threshold for emb in selected_embeddings) \n",
    "\n",
    "Using distance threshold as parameter:\n",
    "distance_ok = all((1-cosine(candidate_embedding, emb))/2.0 >= self.distance_threshold for emb in selected_embeddings) \n",
    "\n",
    "The above two definitions are equivalent:\n",
    "distance_threshold =  1 - similarity_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b529216-7da5-4462-a9ec-b640913deb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152048/2691717285.py:10: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
      "/tmp/ipykernel_152048/2691717285.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = self.retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "Richard Phillips Feynman is credited as one of the founders of quantum physics.\n",
      "0.3\n",
      "Lev Artsimovich\n"
     ]
    }
   ],
   "source": [
    "# This part is for evaluation only\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "\n",
    "class CustomWikipediaRetriever:\n",
    "    def __init__(self, k=3, top_m=10, embedding_model=None):\n",
    "        self.retriever = WikipediaRetriever(top_k_results=top_m)\n",
    "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
    "        self.k = k # Number of documents to sample \n",
    "        self.docs = None\n",
    "        self.doc_embeddings = None\n",
    "    \n",
    "    def retrieve_with_embeddings(self, query):\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\n",
    "        doc_embeddings = [emb/np.linalg.norm(emb) for emb in doc_embeddings]\n",
    "        \n",
    "        self.docs = docs\n",
    "        self.doc_embeddings = doc_embeddings\n",
    "\n",
    "    def retrieve(self, threshold):\n",
    "        if not self.docs:\n",
    "            return []\n",
    "        selected_idx = [random.choice(range(len(self.docs)))]\n",
    "        selected_embeddings = [self.doc_embeddings[selected_idx[0]]]\n",
    "        \n",
    "        remaining_indices = list(set(range(len(self.docs))) - {selected_idx[0]})\n",
    "        while len(selected_idx) < self.k and remaining_indices:\n",
    "            candidate_idx = random.choice(remaining_indices)\n",
    "            candidate_embedding = self.doc_embeddings[candidate_idx]\n",
    "\n",
    "            similarity_ok = all((1+cosine(candidate_embedding, emb))/2.0 < threshold for emb in selected_embeddings)\n",
    "            \n",
    "            if similarity_ok:\n",
    "                selected_idx.append(candidate_idx)\n",
    "                selected_embeddings.append(candidate_embedding)\n",
    "\n",
    "            remaining_indices.remove(candidate_idx)\n",
    "\n",
    "        selected_docs = [self.docs[idx] for idx in selected_idx]\n",
    "        return selected_docs\n",
    "\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, docs=None):\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Answer the question based only on the context provided as short as possible.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        self.docs = docs # for evaluation only\n",
    "        \n",
    "        self.chain = (\n",
    "            {\"context\": self.retrieve_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        ) \n",
    "\n",
    "\n",
    "    def retrieve_docs(self, query):\n",
    "        if not self.docs:\n",
    "            return ''\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in self.docs)\n",
    "\n",
    "    \n",
    "    def answer(self, question: str):\n",
    "        return self.chain.invoke(question)\n",
    "\n",
    "retriever=CustomWikipediaRetriever()\n",
    "query = \"who is the founder of quantum physics\"\n",
    "retriever.retrieve_with_embeddings(query)\n",
    "ls = [0.7,0.3]\n",
    "for th in ls:\n",
    "    print(th)\n",
    "    docs = retriever.retrieve(th)\n",
    "    qa_chain = QAChain(docs=docs)\n",
    "    print(qa_chain.answer(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ddc666f-adaf-4042-a163-136db356ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_test = [0, 0.3, 0.5, 0.6, 0.7, 0.75, 0.8, 0.83, 0.85, 0.88, 0.9, 0.92, 0.95, 0.98]\n",
    "num_sample = 500\n",
    "num_times = 3\n",
    "rng = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bcef18b-7875-4f70-ba05-aa11bdd371f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = EvaluationMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421e1e4f-a0ef-40f5-8726-52a06db0383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataset import get_nq, get_tqa, get_squad, get_asqa\n",
    "\n",
    "nq = get_nq()\n",
    "tqa = get_tqa()\n",
    "squad = get_squad()\n",
    "asqa = get_asqa()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50490e42-eb6a-4ea1-a659-9997b38a017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_ds = {\"NQ\": nq, \"TriviaQA\": tqa, \"SQuAD\": squad, \"ASQA\": asqa}\n",
    "\n",
    "def evaluate(name):\n",
    "    ds = name_to_ds[name]\n",
    "    os.makedirs(\"threshold_results\", exist_ok=True)\n",
    "\n",
    "    \n",
    "    random.seed(rng)\n",
    "    selected_idx = random.sample(range(ds.shape[0]), num_sample)\n",
    "    \n",
    "    n = len(th_test)\n",
    "    \n",
    "    candidates = [[] for _ in range(n)]\n",
    "    references = []\n",
    "    results = []\n",
    "    \n",
    "    for i in selected_idx:\n",
    "        q = ds.loc[i, \"question\"]\n",
    "        a = ds.loc[i, \"answer\"]\n",
    "        retriever = CustomWikipediaRetriever()\n",
    "        retriever.retrieve_with_embeddings(q)\n",
    "        for j in range(n):\n",
    "            th = th_test[j]\n",
    "            c = []\n",
    "            for _ in range(num_times):\n",
    "                docs = retriever.retrieve(th)\n",
    "                qa_chain = QAChain(docs=docs)\n",
    "                answer = qa_chain.answer(q)\n",
    "                c.append(answer)\n",
    "                time.sleep(0.03)\n",
    "                results.append({\"idx in original dataset\":i, \"question\":q , \"similarity threshold\": th, \"num_times\":_, \"retrieved docs\": docs, \"generated answer\": answer, \"standard answer\": a})\n",
    "            candidates[j].append(c) \n",
    "        references.append(a)\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(f'threshold_results/{name}.csv')\n",
    "    \n",
    "    rougeL, diversity = eval.cal_scores(candidates, references)\n",
    "        \n",
    "    with open(f'threshold_results/{name}.txt', 'w') as file:\n",
    "        sys.stdout = file\n",
    "        print(\"The top-k document k are:\", th_test)\n",
    "        print(\"rougeL score for different k is:\", rougeL)\n",
    "        print(\"diversity score for different k is:\", diversity)\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd11a35-467d-4fb4-b94c-418d41e969ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"NQ\", \"TriviaQA\", \"SQuAD\", \"ASQA\"]\n",
    "\n",
    "for ds in datasets:\n",
    "    evaluate(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b454e-09af-45c0-8c18-bd5eb9216805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
