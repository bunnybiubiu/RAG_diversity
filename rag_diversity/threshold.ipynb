{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af1e42-fe3d-40fa-b118-6d7504c58ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your api key\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665884ab-291a-4e72-a681-926d4ad05555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom scipy.spatial.distance import cosine\\nimport random\\n\\nclass CustomWikipediaRetriever:\\n    def __init__(self, k=3, top_m=15, embedding_model=None, threshold=0.7):\\n        self.retriever = WikipediaRetriever(top_k_results=top_m, doc_content_chars_max=2500)\\n        self.embedding_model = embedding_model or OpenAIEmbeddings()\\n        self.k = k # Number of documents to sample \\n        self.docs = None\\n        self.doc_embeddings = None\\n        self.threshold = threshold\\n\\n    def retrieve(self, query):\\n        docs = self.retriever.get_relevant_documents(query)\\n        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\\n        doc_embeddings = [emb / np.linalg.norm(emb) for emb in doc_embeddings]\\n        \\n        selected_idx = [random.choice(range(len(docs)))]\\n        selected_embeddings = [doc_embeddings[selected_idx[0]]]\\n\\n        remaining_indices = list(set(range(len(docs))) - {selected_idx[0]})\\n        while len(selected_idx) < self.k and remaining_indices:\\n            candidate_idx = random.choice(remaining_indices)\\n            candidate_embedding = doc_embeddings[candidate_idx]\\n\\n            similarity_ok = all((1+cosine(candidate_embedding, emb))/2.0 < self.threshold for emb in selected_embeddings)\\n\\n            if similarity_ok:\\n                selected_idx.append(candidate_idx)\\n                selected_embeddings.append(candidate_embedding)\\n\\n            remaining_indices.remove(candidate_idx)\\n\\n        selected_docs = [docs[idx] for idx in selected_idx]\\n        return selected_docs\\n\\n\\nclass QAChain:\\n    def __init__(self, k: int = 3, top_m: int = 15, threshold: float = 0.7, embedding_model=None):\\n        self.retriever = CustomWikipediaRetriever(top_m=top_m, k=k, threshold=threshold, embedding_model=embedding_model)\\n        self.prompt = ChatPromptTemplate.from_template(\\n            \"\"\"Answer the question based only on the context provided as short as possible.\\n\\n            Context: {context}\\n\\n            Question: {question}\"\"\"\\n        )\\n        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\\n        \\n        self.chain = (\\n            {\"context\": self.retrieve_docs, \"question\": RunnablePassthrough()}\\n            | self.prompt\\n            | self.llm\\n            | StrOutputParser()\\n        ) \\n\\n    def retrieve_docs(self, query):\\n        \\n        # for normal RAG pipeline\\n        docs = self.retriever.retrieve(query)\\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\\n    \\n    \\n    def answer(self, question: str):\\n        return self.chain.invoke(question)\\n\\nqa_chain = QAChain(k=3)\\nquery = \"What is the capital of France?\"\\nanswer = qa_chain.answer(query)\\nprint(answer)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This part is for normal RAG pipeline\n",
    "\n",
    "'''\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "\n",
    "class CustomWikipediaRetriever:\n",
    "    def __init__(self, k=3, top_m=15, embedding_model=None, threshold=0.7):\n",
    "        self.retriever = WikipediaRetriever(top_k_results=top_m, doc_content_chars_max=2500)\n",
    "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
    "        self.k = k # Number of documents to sample \n",
    "        self.docs = None\n",
    "        self.doc_embeddings = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\n",
    "        doc_embeddings = [emb / np.linalg.norm(emb) for emb in doc_embeddings]\n",
    "        \n",
    "        selected_idx = [random.choice(range(len(docs)))]\n",
    "        selected_embeddings = [doc_embeddings[selected_idx[0]]]\n",
    "\n",
    "        remaining_indices = list(set(range(len(docs))) - {selected_idx[0]})\n",
    "        while len(selected_idx) < self.k and remaining_indices:\n",
    "            candidate_idx = random.choice(remaining_indices)\n",
    "            candidate_embedding = doc_embeddings[candidate_idx]\n",
    "\n",
    "            similarity_ok = all((1+cosine(candidate_embedding, emb))/2.0 < self.threshold for emb in selected_embeddings)\n",
    "\n",
    "            if similarity_ok:\n",
    "                selected_idx.append(candidate_idx)\n",
    "                selected_embeddings.append(candidate_embedding)\n",
    "\n",
    "            remaining_indices.remove(candidate_idx)\n",
    "\n",
    "        selected_docs = [docs[idx] for idx in selected_idx]\n",
    "        return selected_docs\n",
    "\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, k: int = 3, top_m: int = 15, threshold: float = 0.7, embedding_model=None):\n",
    "        self.retriever = CustomWikipediaRetriever(top_m=top_m, k=k, threshold=threshold, embedding_model=embedding_model)\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Answer the question based only on the context provided as short as possible.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        \n",
    "        self.chain = (\n",
    "            {\"context\": self.retrieve_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        ) \n",
    "\n",
    "    def retrieve_docs(self, query):\n",
    "        \n",
    "        # for normal RAG pipeline\n",
    "        docs = self.retriever.retrieve(query)\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    \n",
    "    def answer(self, question: str):\n",
    "        return self.chain.invoke(question)\n",
    "\n",
    "qa_chain = QAChain(k=3)\n",
    "query = \"What is the capital of France?\"\n",
    "answer = qa_chain.answer(query)\n",
    "print(answer)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0116d09-3c5c-4e10-a160-eed1f209c061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nqa_chain.answer(\"who is the founder of quantum physics\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For normal RAG pipeline\n",
    "'''\n",
    "\n",
    "qa_chain.answer(\"who is the founder of quantum physics\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e629b60-ac5b-4cd7-9a48-badc08f29d2f",
   "metadata": {},
   "source": [
    "Using similarity threshold as parameter:\n",
    "similarity_ok = all((1+cosine(candidate_embedding, emb))/2.0 < self.similarity_threshold for emb in selected_embeddings) \n",
    "\n",
    "Using distance threshold as parameter:\n",
    "distance_ok = all((1-cosine(candidate_embedding, emb))/2.0 >= self.distance_threshold for emb in selected_embeddings) \n",
    "\n",
    "The above two definitions are equivalent:\n",
    "distance_threshold =  1 - similarity_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b529216-7da5-4462-a9ec-b640913deb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is for evaluation only\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "\n",
    "class CustomWikipediaRetriever:\n",
    "    def __init__(self, k=3, top_m=15, embedding_model=None):\n",
    "        self.retriever = WikipediaRetriever(top_k_results=top_m, doc_content_chars_max=2500)\n",
    "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
    "        self.top_m = top_m\n",
    "        self.k = k # Number of documents to sample \n",
    "        self.docs = None\n",
    "        self.doc_embeddings = None\n",
    "    \n",
    "    def retrieve_with_embeddings(self, query):\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\n",
    "        doc_embeddings = [emb/np.linalg.norm(emb) for emb in doc_embeddings]\n",
    "        \n",
    "        self.docs = docs\n",
    "        self.doc_embeddings = doc_embeddings\n",
    "        if not self.docs or len(self.docs)<self.top_m:\n",
    "            return False ## To deal with no sufficient articles in corpus\n",
    "        return True\n",
    "\n",
    "    def retrieve(self, threshold):\n",
    "        if not self.docs:\n",
    "            return []\n",
    "        selected_idx = [random.choice(range(len(self.docs)))]\n",
    "        selected_embeddings = [self.doc_embeddings[selected_idx[0]]]\n",
    "        \n",
    "        remaining_indices = list(set(range(len(self.docs))) - {selected_idx[0]})\n",
    "        while len(selected_idx) < self.k and remaining_indices:\n",
    "            candidate_idx = random.choice(remaining_indices)\n",
    "            candidate_embedding = self.doc_embeddings[candidate_idx]\n",
    "\n",
    "            similarity_ok = all((1-cosine(candidate_embedding, emb)/2.0) < threshold for emb in selected_embeddings)\n",
    "            \n",
    "            if similarity_ok:\n",
    "                selected_idx.append(candidate_idx)\n",
    "                selected_embeddings.append(candidate_embedding)\n",
    "\n",
    "            remaining_indices.remove(candidate_idx)\n",
    "\n",
    "        selected_docs = [self.docs[idx] for idx in selected_idx]\n",
    "        return selected_docs\n",
    "\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, docs=None):\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Answer the question based only on the context provided as short as possible.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        self.docs = docs # for evaluation only\n",
    "        \n",
    "        self.chain = (\n",
    "            {\"context\": self.retrieve_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        ) \n",
    "\n",
    "\n",
    "    def retrieve_docs(self, query):\n",
    "        if not self.docs:\n",
    "            return ''\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in self.docs)\n",
    "\n",
    "    \n",
    "    def answer(self, question: str):\n",
    "        return self.chain.invoke(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ddc666f-adaf-4042-a163-136db356ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_test = [0, 0.3, 0.5, 0.6, 0.7, 0.75, 0.8, 0.83, 0.85, 0.88, 0.9, 0.92, 0.95, 0.98, 1]\n",
    "num_sample = 1000\n",
    "num_times = 3\n",
    "rng = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "421e1e4f-a0ef-40f5-8726-52a06db0383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataset import get_nq, get_tqa, get_squad, get_asqa\n",
    "\n",
    "nq = get_nq()\n",
    "tqa = get_tqa()\n",
    "squad = get_squad()\n",
    "asqa = get_asqa()\n",
    "\n",
    "datasets = [\"NQ\", \"TriviaQA\", \"SQuAD\", \"ASQA\"]\n",
    "\n",
    "name_to_ds = {\"NQ\": nq, \"TriviaQA\": tqa, \"SQuAD\": squad, \"ASQA\": asqa}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155856be-560b-4d8f-825c-dc2f1df1ff0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ\n",
      "selected indices are:  [1309, 228, 51, 1518, 563, 501, 457, 285, 1508, 209, 1385, 1516, 1116, 178, 1209, 864, 65, 61, 191, 447, 476, 1034, 1232, 54, 1149, 407, 1466, 1330, 1436, 1787, 859, 451, 919, 1206, 569, 1657, 13, 1554, 1650, 326, 1429, 865, 696, 1765, 318, 440, 1563, 689, 1790, 189, 778, 198, 735, 1735, 704, 1236, 541, 1652, 88, 1494, 940, 1098, 255, 775, 161, 1130, 600, 1698, 1287, 1266, 740, 1182, 393, 1442, 142, 93, 1354, 466, 1583, 592, 163, 1779, 206, 1749, 1756, 928, 1301, 1708, 747, 333, 758, 727, 429, 1372, 546, 1437, 1399, 1327, 146, 1247, 1300, 350, 1093, 1493, 1794, 334, 946, 777, 552, 1310, 1409, 1140, 449, 1402, 664, 1573, 1589, 114, 469, 1783, 1648, 646, 821, 548, 135, 432, 1161, 1470, 644, 435, 1342, 1022, 810, 1316, 939, 292, 542, 1792, 505, 1525, 1775, 1103, 538, 1529, 1197, 877, 1195, 817, 741, 1687, 283, 1043, 1010, 186, 1547, 96, 224, 313, 1285, 327, 1622, 1393, 1784, 1221, 130, 788, 781, 1220, 958, 1083, 514, 1133, 23, 1638, 1476, 234, 1396, 1099, 1537, 1705, 1574, 1312, 1757, 1798, 601, 890, 323, 929, 6, 1478, 1473, 539, 1025, 1560, 365, 1039, 217, 1280, 611, 1308, 1604, 1700, 1774, 1642, 765, 1561, 330, 1104, 1086, 1, 1226, 663, 1000, 39, 229, 743, 629, 490, 118, 493, 1673, 1746, 175, 1498, 995, 141, 1557, 1090, 1568, 257, 262, 1351, 973, 1125, 338, 1663, 1080, 1242, 866, 433, 1592, 1546, 1740, 1412, 411, 1460, 638, 1742, 1375, 1772, 764, 897, 1059, 924, 247, 507, 460, 131, 692, 43, 1204, 1134, 471, 1205, 1768, 14, 145, 1449, 1292, 120, 468, 138, 64, 676, 1533, 1052, 487, 570, 1370, 994, 438, 1559, 270, 1481, 1169, 1180, 968, 497, 1513, 833, 389, 193, 1748, 1349, 882, 725, 867, 841, 956, 1696, 110, 1379, 1338, 1323, 201, 124, 824, 1491, 694, 223, 509, 392, 1509, 1738, 918, 287, 1637, 375, 1522, 947, 511, 154, 907, 1127, 200, 103, 1335, 1107, 30, 1781, 484, 340, 832, 1520, 985, 437, 1677, 1530, 337, 776, 4, 799, 543, 931, 584, 1594, 1426, 1138, 1355, 996, 317, 388, 607, 445, 119, 1186, 1110, 1684, 642, 117, 102, 1196, 976, 1029, 1087, 322, 116, 1040, 164, 380, 140, 1218, 139, 1382, 481, 826, 245, 1166, 504, 1185, 1217, 81, 1268, 167, 858, 1346, 1653, 1157, 1070, 647, 534, 418, 1371, 643, 488, 1457, 1667, 268, 1551, 1321, 614, 936, 1410, 148, 19, 938, 1272, 1153, 204, 150, 1101, 436, 1036, 1404, 271, 714, 1759, 500, 756, 583, 1613, 1548, 1112, 619, 1252, 1339, 1630, 16, 1367, 1135, 613, 1358, 212, 275, 1743, 236, 219, 1628, 1755, 557, 577, 1238, 431, 702, 416, 1298, 540, 1035, 1587, 1629, 104, 1750, 1299, 1503, 566, 90, 7, 683, 267, 1304, 536, 1593, 904, 1129, 875, 1148, 1395, 1585, 1610, 305, 1117, 73, 1381, 1192, 1131, 303, 880, 261, 85, 631, 746, 1418, 732, 430, 1479, 210, 724, 1146, 1467, 1271, 316, 1469, 332, 362, 844, 50, 367, 680, 843, 508, 1620, 1760, 221, 783, 79, 963, 455, 408, 942, 716, 625, 1722, 456, 48, 395, 816, 672, 1788, 1725, 571, 719, 1679, 818, 1486, 678, 56, 1364, 1689, 1605, 1189, 1386, 78, 222, 1636, 889, 707, 1441, 893, 1241, 1047, 1273, 1634, 1514, 1487, 521, 1344, 1262, 3, 1064, 1102, 403, 745, 883, 143, 1526, 1263, 1737, 615, 1038, 633, 836, 668, 1500, 605, 1521, 260, 1488, 861, 1762, 356, 1165, 616, 831, 1122, 0, 622, 587, 1666, 1496, 1187, 659, 952, 1337, 905, 1464, 1046, 969, 347, 173, 581, 1055, 686, 1672, 1425, 635, 1543, 1597, 301, 1340, 94, 1695, 1567, 149, 932, 848, 1178, 398, 786, 1012, 1277, 499, 302, 11, 218, 870, 448, 360, 1060, 951, 1439, 1141, 510, 248, 934, 273, 1181, 1435, 1144, 649, 906, 1033, 873, 1683, 913, 325, 972, 921, 530, 506, 567, 1067, 992, 489, 562, 900, 158, 585, 480, 556, 687, 654, 1106, 165, 1649, 308, 473, 784, 312, 1519, 1542, 849, 834, 677, 1111, 954, 851, 127, 423, 860, 797, 40, 779, 1704, 12, 720, 1601, 798, 1415, 1250, 1118, 1535, 999, 1761, 558, 892, 1773, 59, 796, 688, 828, 1565, 957, 1322, 1697, 55, 806, 1105, 171, 1654, 277, 945, 372, 1257, 532, 1231, 670, 1606, 1456, 669, 691, 1095, 1715, 863, 516, 1416, 1294, 1586, 106, 1290, 459, 1383, 82, 63, 1579, 1531, 41, 1142, 1405, 258, 1213, 1624, 446, 1428, 524, 755, 343, 1069, 335, 637, 1297, 52, 1553, 768, 812, 406, 155, 1512, 208, 617, 1545, 84, 711, 1389, 1709, 1619, 1387, 698, 25, 1325, 1003, 216, 887, 1651, 941, 1596, 891, 1183, 553, 990, 1159, 1115, 549, 660, 502, 177, 1602, 923, 249, 1057, 475, 1380, 624, 684, 1448, 344, 29, 1076, 871, 1307, 1646, 1270, 1603, 363, 1284, 264, 348, 286, 901, 610, 718, 1002, 282, 1088, 10, 529, 970, 195, 87, 1050, 737, 1451, 1453, 568, 1089, 246, 1264, 1523, 661, 728, 1026, 458, 811, 17, 95, 1202, 226, 414, 708, 1668, 1352, 679, 595, 377, 1600, 1345, 1269, 352, 1670, 763, 1795, 1109, 1515, 1279, 464, 1100, 1114, 1570, 1259, 123, 738, 197, 1576, 122, 760, 1676, 780, 706, 1348, 196, 1728, 1702, 495, 1618, 1037, 603, 1716, 537, 1124, 289, 1097, 852, 1507, 1324, 232, 369, 183, 309, 1534, 1504, 1347, 129, 280, 46, 1658, 965, 299, 1384, 1767, 653, 1265, 770, 980, 105, 1253, 1126, 1223, 291, 1152, 1582, 1119, 1004, 188, 1645, 1071, 1505, 1556, 1433, 1502, 66, 410, 503, 75, 590, 1671, 1356, 1776, 1054, 152, 830, 576, 311, 1158, 254, 121, 1024, 782, 426, 620, 1001, 809, 1240, 231, 794, 535, 1256, 461, 453, 304, 602, 439, 1073, 582, 1536, 1782, 1018, 757, 101, 1633, 1407, 640, 1599, 1517, 1245, 83, 160, 1423, 1607, 565, 76, 845, 2, 1408, 1483, 705, 608, 1766, 298, 33, 237, 295, 723, 1092, 1108, 1501, 72, 703, 239, 1413, 1796, 1403, 850, 935, 1137, 202, 962, 1440, 1360, 230, 1588, 895, 272, 1660, 1328, 856, 169, 314, 609, 766, 1167, 881]\n",
      "\n",
      "\n",
      "TriviaQA\n",
      "selected indices are:  [3597, 1018, 3839, 5641, 2491, 5732, 3297, 7722, 2230, 4099, 4423, 4045, 3586, 658, 4899, 326, 7286, 3539, 6017, 2640, 4945, 2051, 211, 748, 1875, 7879, 5523, 6848, 7049, 4712, 4809, 7788, 169, 6264, 5507, 6729, 2207, 4720, 329, 6251, 6195, 1435, 3854, 4251, 5337, 3622, 7504, 2278, 1486, 4795, 3570, 5200, 6670, 4028, 7938, 747, 3850, 2850, 3345, 2730, 2630, 5489, 856, 7026, 1317, 2701, 3372, 5682, 4058, 2361, 5427, 7743, 3280, 6664, 6230, 4506, 300, 3725, 721, 2576, 2067, 2648, 949, 7947, 6328, 3311, 7085, 4215, 6757, 9, 5387, 7121, 4444, 3784, 3385, 444, 1536, 4247, 2963, 5100, 6196, 4083, 5123, 3621, 6225, 422, 1667, 2187, 4499, 1073, 7599, 2359, 3589, 7215, 5720, 3970, 994, 236, 7972, 5161, 4987, 6547, 1960, 5814, 1297, 2545, 4512, 112, 4524, 3342, 763, 1840, 6892, 7478, 929, 3780, 7758, 962, 5306, 6816, 1261, 4082, 7640, 5870, 2390, 4168, 5778, 2239, 3403, 6838, 3952, 3868, 1996, 3741, 4515, 1184, 3142, 1561, 7551, 4910, 4163, 6113, 7203, 1118, 7078, 571, 2263, 6329, 6471, 6999, 3399, 2784, 7653, 6452, 4159, 2188, 6721, 21, 2317, 5947, 2445, 6860, 4808, 4750, 5406, 4011, 7088, 1217, 3658, 4412, 3967, 2827, 2723, 4521, 6249, 4451, 3090, 3730, 7641, 2636, 7123, 1545, 5713, 1956, 4684, 3137, 1913, 7012, 6353, 3365, 357, 2606, 6100, 3874, 5777, 7482, 6647, 3123, 3162, 5436, 6491, 6723, 5341, 1246, 4057, 7903, 303, 1034, 4114, 7908, 4834, 2719, 7124, 822, 7161, 6922, 3606, 816, 4308, 7458, 3743, 125, 5918, 1180, 3358, 7134, 5363, 7945, 1264, 612, 3846, 6402, 7939, 2171, 2773, 5105, 5674, 3256, 5323, 657, 6977, 2691, 6982, 5524, 7039, 4371, 3113, 7816, 2594, 5134, 5887, 7269, 6221, 3997, 7146, 4432, 294, 5058, 560, 1923, 5170, 5606, 7544, 2354, 1863, 6117, 740, 3555, 806, 6228, 5191, 5766, 7148, 823, 3634, 1362, 5685, 2453, 7400, 237, 376, 2657, 6522, 459, 2403, 2936, 3070, 3528, 1192, 2000, 4351, 3375, 4636, 5585, 6492, 1475, 1392, 1434, 646, 4992, 7133, 3133, 5076, 5596, 1972, 4076, 7477, 4777, 1172, 1902, 3777, 5225, 2080, 3764, 2091, 5462, 76, 7363, 6591, 3811, 7382, 2356, 5550, 4477, 1294, 605, 3618, 2830, 4813, 2450, 5234, 7880, 3475, 5655, 2048, 3742, 6924, 2474, 1631, 3151, 6990, 3958, 873, 1943, 3124, 4685, 2940, 4708, 2423, 5728, 2418, 179, 6799, 5392, 3242, 2248, 66, 7086, 5618, 6375, 7975, 401, 7459, 4967, 6104, 4069, 6822, 7406, 7396, 2344, 6354, 6544, 1885, 4973, 6569, 2886, 1794, 5215, 1557, 5086, 2053, 5552, 6189, 5905, 6287, 5400, 5577, 6867, 1120, 5146, 795, 5140, 5294, 322, 2530, 6460, 3611, 273, 4747, 2988, 5999, 1076, 738, 7451, 2417, 2676, 6123, 1438, 1644, 1082, 6443, 4418, 7179, 7958, 2997, 4348, 4110, 7483, 2232, 6803, 1347, 2105, 7487, 6755, 7734, 3947, 7924, 6606, 6115, 7130, 2774, 6588, 943, 3836, 7888, 616, 1153, 6178, 7917, 1848, 7046, 5540, 5932, 3255, 7923, 6928, 4565, 2996, 739, 6477, 3232, 114, 2166, 4395, 1012, 3019, 5511, 6135, 2147, 4788, 3121, 6737, 5230, 7702, 3043, 887, 5528, 1915, 3862, 205, 5075, 7243, 7726, 4599, 2687, 7497, 4997, 1813, 5305, 517, 5205, 6749, 3803, 7449, 5743, 2475, 5318, 3344, 955, 1145, 371, 7751, 304, 2493, 4035, 951, 796, 7268, 4403, 1111, 3183, 3716, 3039, 5492, 7777, 6087, 5705, 7749, 4425, 3433, 4811, 6080, 5952, 1265, 7253, 3398, 5365, 811, 6829, 4008, 5043, 3343, 7692, 7794, 2291, 268, 5654, 3035, 1779, 3632, 3642, 7786, 1934, 7007, 2971, 813, 5617, 3009, 4460, 7386, 5282, 2938, 496, 3261, 2260, 1554, 7931, 1000, 7774, 6947, 6751, 750, 5429, 1737, 5256, 5238, 4891, 7955, 174, 414, 6445, 2732, 1995, 1031, 6446, 4625, 1681, 562, 6797, 6268, 4540, 1697, 4803, 1769, 6660, 1908, 6343, 1208, 7377, 4882, 23, 2271, 7032, 1185, 1064, 6541, 1429, 900, 5415, 7097, 1079, 121, 2934, 6458, 1949, 4823, 2652, 129, 1427, 2173, 429, 1038, 6076, 3448, 4309, 931, 6108, 520, 3901, 3672, 6373, 2965, 4204, 4863, 893, 3702, 4127, 1814, 7744, 5038, 355, 5957, 6416, 7444, 7102, 5397, 4271, 2470, 3752, 5270, 7897, 255, 498, 3923, 6941, 3290, 3492, 5620, 884, 4016, 5835, 7452, 3633, 602, 7366, 661, 2638, 4983, 1215, 538, 1033, 2252, 5114, 5186, 4492, 5833, 2663, 3120, 4893, 4346, 2415, 4141, 4959, 3524, 812, 6497, 5748, 937, 6986, 5332, 7190, 6299, 4516, 7090, 1761, 3523, 3699, 7275, 1871, 3389, 2776, 6778, 3715, 3266, 3407, 5976, 778, 2560, 3496, 5448, 2088, 3066, 7811, 1250, 5626, 7561, 3885, 549, 6813, 699, 3537, 791, 6099, 6052, 3052, 6650, 1065, 4557, 491, 4804, 7836, 4600, 4601, 2700, 5488, 1001, 2896, 7156, 5450, 7709, 6146, 3464, 7111, 7502, 5906, 421, 7933, 4918, 2559, 2880, 848, 4734, 4156, 1742, 1267, 5379, 3950, 1837, 6940, 886, 2868, 6925, 4556, 3011, 941, 6247, 2282, 4703, 1852, 6612, 3515, 6923, 4595, 7983, 6286, 6704, 5093, 5026, 5529, 5264, 4560, 215, 4988, 7611, 5388, 6794, 5680, 2190, 1477, 2238, 5756, 6245, 2531, 7549, 2783, 2875, 50, 7125, 1173, 4639, 5384, 3283, 570, 1162, 6070, 7961, 251, 751, 6112, 4345, 1762, 3081, 3439, 2792, 1289, 3031, 2552, 5911, 6363, 7712, 4649, 4884, 695, 7236, 430, 1274, 6180, 5061, 407, 5521, 668, 2229, 3629, 5422, 3473, 3978, 3392, 2237, 1765, 6184, 4197, 932, 3521, 908, 2320, 5555, 5558, 4858, 3986, 4316, 5465, 2526, 372, 1806, 3237, 4909, 448, 62, 1674, 2469, 1730, 1124, 6260, 2093, 2371, 982, 63, 4074, 6119, 3527, 1439, 1058, 3114, 4362, 5764, 4098, 4577, 6826, 5472, 6604, 2901, 590, 3252, 7059, 6078, 346, 3573, 153, 3766, 7546, 637, 7062, 2564, 4716, 3516, 4697, 3313, 5812, 5244, 3421, 3318, 170, 7915, 2660, 1407, 6564, 7762, 5063, 3769, 6815, 5649, 7534, 2964, 3577, 6918, 867, 1993, 3569, 4824, 4292, 644, 7131, 2541, 6109, 2781, 1815, 2728, 6382, 1377, 625, 4181, 5187, 934, 4177, 1588, 7421, 6358, 2862, 2876, 5958, 7849, 6708, 5287, 6676, 1209, 1935, 842, 1199, 2096, 1616, 1421, 4934, 1252, 6227, 6218, 5369, 617, 1451, 7792, 6331, 5144, 4047, 3800, 4618, 6224, 3677, 5579, 7570, 7225, 4624, 5267, 5204, 5116, 2647, 7079, 7954, 5138, 2589, 1236, 3602, 559, 3841, 5172, 2480, 6523, 2250, 4844, 460, 2883, 607, 2542, 3782, 308, 466, 3020, 2351, 628, 5281, 7006, 4866]\n",
      "\n",
      "\n",
      "SQuAD\n",
      "selected indices are:  [8307, 6299, 7581, 9508, 9081, 672, 7368, 9366, 3084, 5267, 9912, 7792, 8214, 2471, 1013, 7381, 1695, 5626, 1381, 8266, 2827, 641, 4059, 7172, 7199, 8586, 8564, 9991, 2600, 5962, 6108, 4634, 6347, 6697, 5543, 9795, 857, 10336, 5482, 1079, 5401, 1548, 9141, 6333, 4655, 4128, 9874, 2463, 5461, 1335, 9547, 2317, 5731, 5082, 6421, 2112, 9749, 1388, 5072, 9157, 6171, 10542, 5381, 2093, 8624, 1531, 6937, 8331, 5928, 298, 5940, 5060, 2953, 3509, 5598, 7967, 3145, 3711, 2253, 2538, 1264, 4846, 1657, 8317, 8843, 8626, 618, 5517, 10125, 2147, 9785, 6172, 2527, 2658, 2962, 10235, 2712, 7170, 715, 6731, 5968, 3891, 7277, 10003, 4668, 7355, 3833, 8749, 3918, 5070, 7684, 3178, 6026, 9345, 7218, 7564, 4616, 6256, 8239, 8641, 6859, 2655, 3271, 9906, 2267, 4096, 854, 10505, 7873, 6081, 9085, 1680, 8452, 2042, 4670, 1374, 2626, 4469, 7361, 8410, 2414, 7167, 1502, 3637, 7391, 5727, 436, 6797, 872, 6495, 8224, 6126, 3862, 6326, 1337, 6142, 3678, 461, 5221, 1623, 5493, 2392, 2254, 627, 4700, 7740, 2273, 7685, 7349, 10081, 86, 1298, 311, 4193, 3533, 2449, 8991, 9975, 8647, 6934, 1821, 4720, 3893, 4934, 1996, 782, 3906, 6878, 10468, 10204, 7489, 1029, 1816, 8189, 9776, 8782, 269, 10353, 8443, 9421, 3963, 2352, 4772, 7032, 25, 10073, 5780, 3941, 9349, 6825, 3068, 1402, 8576, 5908, 1108, 8618, 8914, 8312, 8316, 9079, 333, 6397, 7702, 713, 10414, 6338, 6116, 4155, 266, 5850, 1106, 5649, 3950, 10293, 1698, 9533, 5447, 2185, 726, 5772, 8945, 5546, 10531, 2868, 7612, 7840, 10350, 2986, 2209, 1033, 7498, 605, 4807, 3302, 717, 3268, 686, 5170, 5080, 8444, 6524, 8897, 7756, 4150, 599, 3131, 4687, 5848, 783, 5438, 4476, 2039, 6029, 7159, 6553, 7204, 6334, 5554, 3060, 8129, 8151, 6018, 8507, 4369, 1353, 6955, 1293, 7055, 9870, 2956, 8937, 4813, 5262, 1311, 5372, 4843, 5021, 7305, 9873, 6981, 2730, 7272, 5759, 7325, 693, 5776, 7126, 4498, 10477, 939, 1229, 10445, 6655, 5952, 8406, 2621, 510, 2339, 9954, 7179, 570, 2068, 1101, 3866, 6001, 5933, 6272, 9293, 530, 9913, 2513, 7371, 6077, 6095, 7273, 1259, 9403, 2255, 8674, 6012, 6521, 5150, 4566, 4089, 1858, 424, 3048, 8176, 8482, 6340, 9202, 1929, 4288, 4264, 7311, 3515, 10024, 4678, 8047, 3279, 2009, 2223, 1213, 7406, 2828, 7292, 1438, 5236, 5692, 1063, 9011, 8882, 4757, 4914, 2580, 10448, 2852, 5922, 8335, 3673, 1988, 3292, 2275, 3878, 8094, 430, 5912, 9077, 9376, 6044, 7657, 9038, 2126, 10026, 1412, 1077, 5067, 6523, 7842, 8612, 6730, 6708, 9419, 1208, 2053, 5194, 10524, 1215, 7372, 7632, 8476, 5644, 2102, 9032, 10482, 9630, 2981, 2114, 7088, 8238, 907, 2034, 8488, 2507, 4982, 2697, 2653, 5286, 3694, 5669, 8501, 4653, 1292, 4105, 3216, 10404, 9024, 4496, 2050, 10240, 4961, 10066, 8740, 1530, 8234, 10501, 2762, 9696, 9513, 2526, 2804, 10231, 9914, 5531, 9234, 674, 464, 1330, 744, 10509, 9449, 4336, 3453, 9373, 10127, 10469, 496, 8159, 10275, 8938, 4745, 10517, 4949, 7912, 4012, 6653, 4873, 7430, 1195, 981, 2589, 7202, 6809, 7933, 7609, 3342, 5573, 9939, 2353, 5121, 5231, 5658, 6532, 2142, 6070, 8437, 1739, 5228, 3960, 7642, 2007, 4382, 7366, 4061, 2308, 1586, 829, 4754, 6293, 10082, 6850, 4067, 2616, 5364, 9464, 5123, 3110, 2610, 8163, 8433, 7648, 8171, 5055, 8148, 379, 1475, 6442, 8281, 7491, 3944, 3524, 9557, 5781, 797, 828, 4609, 8108, 9789, 7710, 4681, 8793, 133, 1761, 7060, 2194, 4333, 5992, 6606, 5995, 741, 6563, 838, 9343, 9204, 3189, 5939, 9064, 4728, 1204, 6329, 8261, 7378, 9008, 4582, 10211, 10011, 1946, 2108, 1584, 6453, 6114, 5555, 9139, 5990, 2363, 3261, 8341, 6576, 8193, 659, 742, 637, 2245, 5456, 7761, 8508, 7487, 2440, 9932, 8446, 2290, 5373, 10044, 5218, 2662, 6438, 10100, 4902, 9727, 5511, 8311, 8351, 8721, 8025, 9222, 4911, 7780, 272, 6035, 5427, 1795, 6824, 9563, 5040, 10309, 9779, 7775, 4350, 9477, 9463, 3728, 841, 9560, 7871, 2793, 8589, 10305, 10153, 6229, 2421, 3968, 517, 9380, 1800, 3125, 310, 7225, 5139, 6861, 2481, 6765, 3340, 6723, 8221, 10018, 7726, 1020, 2262, 8498, 3397, 9189, 5325, 7837, 8609, 6173, 5140, 2837, 7531, 8734, 5608, 8950, 5804, 10539, 4331, 9995, 7932, 3150, 4038, 4571, 9143, 4890, 3684, 4876, 4735, 3396, 8011, 7861, 5714, 9183, 4480, 4715, 9395, 8899, 6220, 6464, 5654, 2399, 4758, 689, 4712, 1294, 5674, 7247, 4201, 7849, 3505, 3310, 8824, 4442, 9208, 4450, 2249, 1789, 10087, 9614, 3920, 3972, 831, 8700, 3697, 10452, 3814, 860, 1641, 6771, 5411, 7739, 1647, 2252, 84, 9016, 2587, 6666, 7793, 7822, 3267, 4706, 4676, 969, 1465, 9406, 3804, 8767, 611, 6846, 2880, 595, 6504, 8116, 3052, 4742, 613, 151, 4888, 9305, 9875, 1758, 5489, 4662, 7444, 10507, 8901, 8590, 8092, 2198, 7668, 4470, 3159, 1845, 5417, 7518, 4215, 3050, 230, 5523, 4833, 9302, 3164, 2875, 10009, 10464, 6643, 7007, 8442, 1421, 6567, 1562, 3027, 2303, 5301, 4064, 111, 4272, 6284, 3858, 7319, 4374, 5400, 4947, 9543, 9386, 187, 4285, 5890, 3871, 1017, 1936, 7625, 5026, 6642, 8233, 5095, 1923, 6021, 10070, 3618, 3590, 2188, 7834, 2509, 7454, 9924, 6121, 6813, 8998, 7714, 8806, 3578, 4054, 9761, 1341, 8653, 5927, 1279, 9243, 1838, 1011, 8972, 3312, 9383, 8792, 2455, 2695, 5375, 8525, 7239, 1904, 3365, 9554, 8008, 1489, 8361, 7298, 911, 7429, 2161, 8409, 6806, 7484, 9239, 945, 9155, 7575, 5049, 356, 6487, 4169, 49, 3570, 9475, 1196, 6941, 1044, 8866, 987, 1130, 7734, 519, 4703, 6698, 2948, 2218, 10510, 6887, 6135, 6266, 7348, 6184, 6154, 1314, 8835, 2178, 5697, 1943, 2924, 8802, 6439, 8664, 2086, 3648, 56, 372, 7585, 8920, 6948, 8715, 6214, 3762, 4055, 7546, 5671, 2541, 4516, 3088, 1848, 527, 6868, 10074, 256, 3943, 3376, 1102, 1653, 9731, 550, 7314, 9794, 4009, 724, 6590, 7191, 3839, 3559, 924, 2295, 8255, 4739, 3836, 9438, 5224, 9461, 9796, 5253, 3881, 4944, 2347, 8540, 3621, 6775, 4923, 4508, 998, 9121, 9693, 2869, 10256, 6999, 9107, 8120, 768, 5642, 10540, 6244, 8592, 5222, 6821, 6688, 2444, 4913, 6167, 3012, 8813, 7758, 3948, 3692, 4927, 2365, 7586, 944, 9214, 6761, 9130, 8683, 2196, 6363, 3978, 4177, 3331, 1295, 7369, 6079, 1514, 8775, 3120, 844, 4398, 6179, 9911, 9878, 647, 1192, 3082, 9626, 9156, 3555, 7854, 3422, 5455, 4967, 251]\n",
      "\n",
      "\n",
      "ASQA\n",
      "selected indices are:  [867, 778, 3037, 480, 3059, 3089, 1961, 993, 2849, 2475, 2882, 837, 1624, 980, 2261, 1317, 1159, 1559, 1909, 2185, 2657, 1471, 1263, 1071, 1473, 2100, 2036, 1908, 403, 2959, 1923, 3119, 1311, 832, 1518, 1281, 1695, 187, 2304, 906, 3036, 597, 66, 1068, 2266, 2392, 2372, 2949, 1710, 1209, 624, 803, 1351, 942, 1555, 2333, 1006, 2045, 2255, 2681, 2805, 1382, 1054, 2001, 2963, 2628, 3029, 2009, 1887, 689, 3001, 1442, 693, 573, 2950, 2238, 2000, 753, 2220, 3118, 240, 2146, 138, 304, 2737, 198, 26, 1689, 563, 2583, 947, 278, 2896, 618, 37, 895, 2071, 1867, 1528, 247, 2530, 2612, 2731, 2525, 1979, 2700, 1998, 64, 27, 2179, 2259, 1684, 48, 68, 2168, 2958, 1125, 2194, 1174, 70, 2057, 2858, 2760, 1763, 734, 438, 394, 2147, 609, 987, 786, 2538, 2156, 1033, 1451, 1094, 1625, 323, 3085, 1663, 1880, 2314, 996, 2855, 925, 1228, 2801, 330, 2677, 2661, 129, 382, 1660, 1553, 1544, 2263, 1949, 229, 2608, 38, 2878, 701, 338, 2047, 1777, 2641, 1356, 2317, 390, 2163, 174, 941, 865, 2835, 2321, 1947, 1110, 190, 3100, 2803, 1148, 2224, 2308, 2695, 134, 3179, 1288, 3076, 848, 2404, 590, 2919, 1631, 315, 1225, 665, 2309, 984, 3009, 1593, 2773, 2214, 1358, 1572, 3110, 2823, 2966, 321, 2051, 1416, 219, 400, 1790, 950, 311, 1395, 2478, 2514, 2439, 3171, 1336, 121, 2597, 1119, 1845, 3116, 934, 1457, 2265, 1537, 1768, 760, 2784, 2399, 2706, 1556, 351, 2533, 3134, 1009, 2920, 301, 336, 1097, 631, 2948, 2915, 2596, 2939, 1595, 1297, 1477, 436, 374, 21, 1262, 1821, 1474, 1104, 418, 542, 357, 767, 1765, 1838, 2281, 2272, 2098, 1671, 423, 107, 367, 1449, 2270, 381, 2445, 2449, 1328, 1577, 50, 1195, 1694, 1588, 346, 2291, 994, 2342, 2134, 694, 2807, 3166, 2895, 568, 1101, 1233, 1099, 2019, 3142, 259, 684, 1780, 1130, 1724, 1227, 1983, 2990, 1479, 1030, 1010, 2560, 2030, 2432, 3080, 800, 1875, 3058, 554, 1247, 3075, 1620, 1360, 2540, 2938, 1349, 1803, 1371, 3060, 2666, 2434, 557, 1230, 1313, 2470, 2847, 822, 1962, 1286, 727, 2991, 1307, 1193, 2592, 2011, 2358, 992, 1334, 1540, 3003, 1609, 1495, 466, 2987, 818, 2425, 2234, 736, 2792, 2253, 110, 1890, 858, 1795, 2843, 283, 1675, 2756, 3126, 570, 2594, 1240, 991, 1034, 2697, 2935, 1733, 3117, 302, 1839, 2907, 2848, 2384, 1641, 2188, 2075, 1713, 2226, 150, 1472, 2199, 2872, 2613, 348, 447, 1016, 2693, 2722, 1454, 681, 2650, 2509, 181, 2306, 2642, 2774, 2649, 1636, 2981, 2857, 433, 43, 3155, 1059, 909, 2091, 2124, 2285, 2377, 2356, 905, 1825, 1532, 1602, 1896, 2410, 2049, 617, 1412, 100, 1977, 428, 1210, 1703, 2947, 475, 580, 1426, 1277, 1405, 1865, 847, 3091, 1978, 2748, 1950, 399, 1794, 1853, 1306, 275, 1229, 183, 471, 92, 2746, 2654, 2791, 673, 999, 2113, 714, 2260, 655, 1355, 2293, 1746, 1892, 949, 1659, 2579, 755, 763, 2623, 2692, 1770, 2864, 119, 2515, 808, 1841, 2830, 1759, 1592, 18, 879, 842, 1140, 256, 2364, 416, 2795, 765, 1498, 1335, 807, 1874, 2833, 1074, 2004, 2161, 2617, 1283, 2908, 1591, 2503, 1607, 2994, 467, 1425, 1441, 2869, 2528, 706, 1218, 2516, 2417, 347, 548, 1282, 482, 977, 1250, 479, 749, 1529, 579, 2092, 3124, 2799, 2442, 564, 2354, 1571, 1732, 2952, 1993, 2603, 2201, 2781, 709, 2917, 683, 2005, 1183, 574, 766, 2997, 1847, 2537, 2973, 1470, 39, 1984, 559, 799, 2980, 2300, 2070, 2027, 2819, 2014, 1702, 1818, 3120, 685, 3021, 2315, 126, 903, 1196, 133, 3067, 920, 2202, 1182, 688, 1873, 2316, 2028, 2251, 2658, 463, 2345, 3035, 2940, 3002, 1502, 3105, 170, 1807, 2237, 3088, 1741, 419, 1013, 1219, 132, 1842, 1076, 1427, 356, 1798, 3180, 970, 859, 2408, 1436, 2499, 1753, 679, 2523, 2816, 846, 2744, 243, 2335, 2578, 2180, 1151, 2444, 2206, 3114, 1325, 1198, 1185, 2602, 1095, 2107, 3015, 555, 1685, 242, 1135, 519, 543, 1022, 599, 1319, 2590, 1608, 2686, 582, 2365, 1100, 1696, 1536, 1851, 309, 2909, 1650, 2111, 1139, 1511, 1862, 1999, 1340, 2383, 10, 2536, 1877, 1467, 2885, 2189, 1629, 892, 2703, 3183, 2026, 2888, 1322, 1161, 1383, 2241, 2367, 534, 2318, 1987, 1407, 3098, 188, 407, 1882, 3141, 508, 647, 2595, 1869, 2, 1752, 828, 540, 1236, 651, 1122, 3032, 1476, 1025, 337, 1523, 677, 212, 1623, 1251, 959, 1758, 373, 389, 4, 874, 1953, 319, 2550, 2431, 926, 2127, 1822, 3024, 33, 2507, 493, 1729, 541, 1956, 290, 937, 1569, 365, 2591, 422, 2683, 1505, 2989, 561, 1567, 545, 585, 280, 2172, 2307, 34, 672, 1802, 1438, 872, 620, 1686, 1812, 889, 355, 411, 2979, 504, 2414, 2629, 1439, 2480, 2638, 572, 1014, 1134, 345, 2790, 3139, 1167, 109, 1235, 841, 2120, 2087, 772, 1604, 2753, 223, 3170, 2626, 1580, 461, 3054, 2046, 2461, 3014, 49, 2877, 1296, 2494, 1584, 2328, 1723, 1496, 2815, 707, 1929, 298, 3070, 276, 56, 1073, 883, 163, 3084, 1630, 2800, 1173, 2052, 1701, 1734, 1647, 2487, 2197, 2611, 625, 1136, 342, 1269, 3046, 2094, 3150, 636, 2376, 1338, 1600, 270, 1270, 1791, 2664, 237, 1001, 353, 1779, 2750, 1857, 225, 1266, 719, 489, 51, 562, 46, 671, 2033, 1420, 2142, 2118, 1061, 2643, 2486, 2551, 2541, 483, 120, 1372, 2429, 1114, 2145, 261, 1077, 3023, 2032, 1864, 2604, 1475, 227, 2406, 2985, 2340, 1506, 644, 1044, 421, 472, 935, 2084, 2527, 180, 2904, 2840, 2505, 1932, 1490, 1568, 614, 733, 144, 2264, 1717, 919, 1318, 2423, 1704, 1310, 1111, 2474, 1527, 2350, 2056, 215, 2998, 914, 2112, 3113, 1642, 282, 1905, 1156, 1272, 2529, 2357, 2955, 1870, 32, 2532, 821, 1194, 1244, 3176, 1900, 1508, 2020, 2870, 3108, 1004, 616, 3097, 1679, 97, 954, 2218, 1418, 2633, 2334, 5, 1541, 1259, 425, 840, 2170, 975, 1719, 2015, 245, 583, 1146, 2526, 179, 948, 2128, 1691, 1525, 2871, 2668, 391, 2627, 546, 1615, 306, 255, 1781, 535, 969, 2644, 1069, 1290, 3010, 3152, 1302, 1854, 1113, 956, 305, 830, 2854, 446, 2511, 432, 2727, 2705, 3156, 2850, 1665, 494, 2195, 2524, 850, 3052, 2928, 1897, 1085, 491, 370, 645, 1246, 165, 880, 1342, 607, 2479, 2297, 2956, 2483, 2103, 200]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indices = {}\n",
    "random.seed(rng)\n",
    "    \n",
    "for name in datasets:\n",
    "    ds = name_to_ds[name]\n",
    "    print(name)\n",
    "    selected_idx = random.sample(range(ds.shape[0]), num_sample)\n",
    "    indices[name] = selected_idx\n",
    "    print(\"selected indices are: \", selected_idx)\n",
    "    print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50490e42-eb6a-4ea1-a659-9997b38a017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(name, selected_idx):\n",
    "    \n",
    "    ds = name_to_ds[name]\n",
    "    os.makedirs(f\"threshold_results/{name}\", exist_ok=True)\n",
    "\n",
    "    print(name)\n",
    "    print(\"evaluation progress: \", end=\"\")\n",
    "    \n",
    "    n = len(th_test)\n",
    "\n",
    "    candidates = [[] for _ in range(n)]\n",
    "    references = []\n",
    "\n",
    "    with open(f'threshold_results/{name}/references.jsonl', 'a') as ref_file:\n",
    "        \n",
    "        # Open separate candidate files for each j\n",
    "        cand_files = [open(f'threshold_results/{name}/cand_{j}.jsonl', 'a') for j in range(n)]\n",
    "        \n",
    "        for idx in selected_idx:\n",
    "            print(idx, end=\", \")\n",
    "            q = ds.loc[idx, \"question\"]\n",
    "            a = ds.loc[idx, \"answer\"]\n",
    "            try:\n",
    "                retriever = CustomWikipediaRetriever()\n",
    "                if not retriever.retrieve_with_embeddings(q):\n",
    "                    continue\n",
    "                for j in range(n):\n",
    "                    th = th_test[j]\n",
    "                    c = []\n",
    "                    for _ in range(num_times):\n",
    "                        docs = retriever.retrieve(th)\n",
    "                        qa_chain = QAChain(docs=docs)\n",
    "                        answer = qa_chain.answer(q)\n",
    "                        c.append(answer)\n",
    "                        time.sleep(0.03)\n",
    "                        \n",
    "                    candidates[j].append(c) \n",
    "                    cand_files[j].write(f\"{c}\\n\")\n",
    "                    cand_files[j].flush()\n",
    "                    \n",
    "                references.append(a)\n",
    "                ref_file.write(f\"{a}\\n\")\n",
    "                ref_file.flush()\n",
    "            except: ## To deal with fetching API too frequently\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    retriever = CustomWikipediaRetriever()\n",
    "                    if not retriever.retrieve_with_embeddings(q): \n",
    "                        continue\n",
    "                    for j in range(n):\n",
    "                        th = th_test[j]\n",
    "                        c = []\n",
    "                        for _ in range(num_times):\n",
    "                            docs = retriever.retrieve(th)\n",
    "                            qa_chain = QAChain(docs=docs)\n",
    "                            answer = qa_chain.answer(q)\n",
    "                            c.append(answer)\n",
    "                            time.sleep(0.03)\n",
    "\n",
    "                        candidates[j].append(c) \n",
    "                        cand_files[j].write(f\"{c}\\n\")\n",
    "                        cand_files[j].flush()\n",
    "                        \n",
    "                    references.append(a)\n",
    "                    ref_file.write(f\"{a}\\n\")\n",
    "                    ref_file.flush()\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        for file in cand_files:\n",
    "            file.close()\n",
    "        ref_file.close()      \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd11a35-467d-4fb4-b94c-418d41e969ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ\n",
      "evaluation progress: 1309, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1283315/1289903426.py:10: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
      "/tmp/ipykernel_1283315/1289903426.py:17: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = self.retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228, 51, 1518, 563, 501, 457, 285, 1508, 209, 1385, 1516, 1116, 178, 1209, 864, 65, 61, 191, 447, 476, 1034, 1232, 54, 1149, 407, 1466, 1330, 1436, 1787, 859, 451, 919, 1206, 569, 1657, 13, 1554, 1650, 326, 1429, 865, 696, 1765, 318, 440, 1563, 689, 1790, 189, 778, 198, 735, 1735, 704, 1236, 541, 1652, 88, 1494, 940, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/e1331099/anaconda3/lib/python3.12/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/e/e1331099/anaconda3/lib/python3.12/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098, 255, 775, 161, 1130, 600, 1698, 1287, 1266, 740, 1182, 393, 1442, 142, 93, 1354, 466, 1583, 592, 163, 1779, 206, 1749, 1756, 928, 1301, 1708, 747, 333, 758, 727, 429, 1372, 546, 1437, 1399, 1327, 146, 1247, 1300, 350, 1093, 1493, 1794, 334, 946, 777, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/e1331099/anaconda3/lib/python3.12/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/e/e1331099/anaconda3/lib/python3.12/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552, 1310, 1409, 1140, 449, 1402, 664, 1573, 1589, 114, 469, 1783, 1648, 646, 821, 548, 135, 432, 1161, 1470, 644, 435, 1342, 1022, 810, 1316, 939, 292, 542, 1792, 505, 1525, 1775, 1103, 538, 1529, 1197, 877, 1195, 817, 741, 1687, 283, 1043, 1010, 186, 1547, 96, 224, 313, 1285, 327, 1622, 1393, 1784, 1221, 130, 788, 781, 1220, 958, 1083, 514, 1133, 23, 1638, 1476, 234, 1396, 1099, 1537, 1705, 1574, 1312, 1757, 1798, 601, 890, 323, 929, 6, 1478, 1473, 539, 1025, 1560, 365, 1039, 217, 1280, 611, 1308, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ds in datasets:\n",
    "    evaluate(ds, indices[ds])\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b454e-09af-45c0-8c18-bd5eb9216805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5322632-a3f3-4d4c-a179-142848011fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
