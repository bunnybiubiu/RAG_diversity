{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f1ab2f1-0f0a-405a-9986-e5ded2a4f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from evaluation import EvaluationMetrics\n",
    "import time\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8764f4f-a63f-4492-9a11-31fdc4ead1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom scipy.spatial.distance import cosine\\n\\nclass CustomWikipediaRetriever:\\n    def __init__(self, k=3, top_m=10, temperature=1.0, std_error=0.0, embedding_model=None):\\n        self.retriever = WikipediaRetriever(top_k_results=top_m)\\n        self.embedding_model = embedding_model or OpenAIEmbeddings()\\n        self.k = k  # Number of documents to sample\\n        self.temperature = temperature  # Temperature scaling for softmax\\n        self.std_error = std_error\\n\\n    def softmax(self, similarities, temperature):\\n        \"\"\"Apply softmax with temperature to convert similarities into probabilities.\"\"\"\\n        e_similarities = np.exp(np.array(similarities) / temperature)\\n        return e_similarities / np.sum(e_similarities)\\n\\n    def add_noise_to_probabilities(self, probabilities):\\n        \"\"\"Add normal distributed noise to the probabilities.\"\"\"\\n        noise = np.random.normal(0, self.std_error, len(probabilities))\\n        noisy_probabilities = probabilities + noise\\n        \\n        # Ensure all probabilities are non-negative\\n        noisy_probabilities = np.maximum(noisy_probabilities, 0)\\n        \\n        # Normalize the probabilities so they sum to 1\\n        return noisy_probabilities / np.sum(noisy_probabilities)\\n\\n    def retrieve_with_similarity(self, query):\\n        # Get top-m documents from the retriever\\n        docs = self.retriever.get_relevant_documents(query)\\n        \\n        # Calculate the similarity scores\\n        query_embedding = self.embedding_model.embed_query(query)\\n        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\\n        \\n        # Calculate cosine similarity scores\\n        similarities = [1 - cosine(query_embedding, doc_emb) for doc_emb in doc_embeddings]\\n        \\n        # Convert similarities to probabilities\\n        probabilities = self.softmax(similarities, self.temperature)\\n\\n        noisy_probabilities = self.add_noise_to_probabilities(probabilities)\\n\\n        # Randomly sample k documents from the top-m using probabilities\\n        sampled_indices = np.random.choice(len(docs), size=self.k, replace=False, p=noisy_probabilities)\\n        results = [docs[i] for i in sampled_indices]\\n\\n        return results\\n\\n\\nclass QAChain:\\n    def __init__(self, k: int = 3, top_m: int = 10, temperature=1.0, std_error=0.0, embedding_model=None):\\n        \"\"\"\\n        Initialize the QAChain class with the desired number of retrieved documents (k).\\n\\n        Parameters:\\n        - k: Number of documents to retrieve from Wikipedia for context (default is 3).\\n        - embedding_model: The embedding model to use (default is OpenAIEmbeddings).\\n        \"\"\"\\n        self.retriever = CustomWikipediaRetriever(k=k, top_m=top_m, temperature=temperature, std_error=std_error, embedding_model=embedding_model)\\n\\n        # Define the prompt template\\n        self.prompt = ChatPromptTemplate.from_template(\\n            \"\"\"Answer the question based only on the context provided as short as possible.\\n\\n            Context: {context}\\n\\n            Question: {question}\"\"\"\\n        )\\n        \\n        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\\n\\n        # Create the full chain\\n        self.chain = (\\n            {\"context\": self.retrieve_docs,  # Fixed method reference\\n             \"question\": RunnablePassthrough()}\\n            | self.prompt\\n            | self.llm\\n            | StrOutputParser()\\n        )\\n\\n    def retrieve_docs(self, query):\\n        \"\"\"\\n        Retrieve and format documents from Wikipedia for the chain.\\n        \"\"\"\\n        docs = self.retriever.retrieve_with_similarity(query)\\n        return \"\\n\\n\".join(doc.page_content for doc in docs)\\n    \\n    def answer(self, question: str):\\n        \"\"\"\\n        Answer a given question based on the context retrieved from Wikipedia.\\n\\n        Parameters:\\n        - question: The question to ask.\\n        \"\"\"\\n        return self.chain.invoke(question)\\n\\n\\n# Instantiate the QAChain without explicitly passing an embedding model\\nqa_chain = QAChain(k=3)\\n\\n# Retrieve documents with similarity scores and provide the answer\\nquery = \"What is the capital of France?\"\\nanswer = qa_chain.answer(query)\\n\\n# Print the answer\\nprint(answer)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for normal RAG pipeline\n",
    "'''\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "class CustomWikipediaRetriever:\n",
    "    def __init__(self, k=3, top_m=10, temperature=1.0, std_error=0.0, embedding_model=None):\n",
    "        self.retriever = WikipediaRetriever(top_k_results=top_m)\n",
    "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
    "        self.k = k  # Number of documents to sample\n",
    "        self.temperature = temperature  # Temperature scaling for softmax\n",
    "        self.std_error = std_error\n",
    "\n",
    "    def softmax(self, similarities, temperature):\n",
    "        \"\"\"Apply softmax with temperature to convert similarities into probabilities.\"\"\"\n",
    "        e_similarities = np.exp(np.array(similarities) / temperature)\n",
    "        return e_similarities / np.sum(e_similarities)\n",
    "\n",
    "    def add_noise_to_probabilities(self, probabilities):\n",
    "        \"\"\"Add normal distributed noise to the probabilities.\"\"\"\n",
    "        noise = np.random.normal(0, self.std_error, len(probabilities))\n",
    "        noisy_probabilities = probabilities + noise\n",
    "        \n",
    "        # Ensure all probabilities are non-negative\n",
    "        noisy_probabilities = np.maximum(noisy_probabilities, 0)\n",
    "        \n",
    "        # Normalize the probabilities so they sum to 1\n",
    "        return noisy_probabilities / np.sum(noisy_probabilities)\n",
    "\n",
    "    def retrieve_with_similarity(self, query):\n",
    "        # Get top-m documents from the retriever\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # Calculate the similarity scores\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\n",
    "        \n",
    "        # Calculate cosine similarity scores\n",
    "        similarities = [1 - cosine(query_embedding, doc_emb) for doc_emb in doc_embeddings]\n",
    "        \n",
    "        # Convert similarities to probabilities\n",
    "        probabilities = self.softmax(similarities, self.temperature)\n",
    "\n",
    "        noisy_probabilities = self.add_noise_to_probabilities(probabilities)\n",
    "\n",
    "        # Randomly sample k documents from the top-m using probabilities\n",
    "        sampled_indices = np.random.choice(len(docs), size=self.k, replace=False, p=noisy_probabilities)\n",
    "        results = [docs[i] for i in sampled_indices]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, k: int = 3, top_m: int = 10, temperature=1.0, std_error=0.0, embedding_model=None):\n",
    "        \"\"\"\n",
    "        Initialize the QAChain class with the desired number of retrieved documents (k).\n",
    "\n",
    "        Parameters:\n",
    "        - k: Number of documents to retrieve from Wikipedia for context (default is 3).\n",
    "        - embedding_model: The embedding model to use (default is OpenAIEmbeddings).\n",
    "        \"\"\"\n",
    "        self.retriever = CustomWikipediaRetriever(k=k, top_m=top_m, temperature=temperature, std_error=std_error, embedding_model=embedding_model)\n",
    "\n",
    "        # Define the prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Answer the question based only on the context provided as short as possible.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "        # Create the full chain\n",
    "        self.chain = (\n",
    "            {\"context\": self.retrieve_docs,  # Fixed method reference\n",
    "             \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def retrieve_docs(self, query):\n",
    "        \"\"\"\n",
    "        Retrieve and format documents from Wikipedia for the chain.\n",
    "        \"\"\"\n",
    "        docs = self.retriever.retrieve_with_similarity(query)\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    def answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Answer a given question based on the context retrieved from Wikipedia.\n",
    "\n",
    "        Parameters:\n",
    "        - question: The question to ask.\n",
    "        \"\"\"\n",
    "        return self.chain.invoke(question)\n",
    "\n",
    "\n",
    "# Instantiate the QAChain without explicitly passing an embedding model\n",
    "qa_chain = QAChain(k=3)\n",
    "\n",
    "# Retrieve documents with similarity scores and provide the answer\n",
    "query = \"What is the capital of France?\"\n",
    "answer = qa_chain.answer(query)\n",
    "\n",
    "# Print the answer\n",
    "print(answer)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b324c64e-45d5-4dd9-8ea0-06d2007bb85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1127154/875525325.py:32: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = self.retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nikolay Storonsky (born 21 July 1984) is a Russian-born British entrepreneur. He is best known as th\n",
      "Pyotr Kapitsa\n",
      "Fotini G. Markopoulou-Kalamara (Greek: Φωτεινή Μαρκοπούλου-Καλαμαρά; born April 3, 1971) is a Greek \n",
      "Yakov Frenkel and Matvei Petrovich Bronstein are notable founders of quantum physics.\n",
      "Guillaume Verdon-Akzam, also known as Guillaume Verdon, or Gill Verdon is a Canadian mathematical ph\n",
      "The founder of quantum physics is not mentioned in the provided context.\n"
     ]
    }
   ],
   "source": [
    "# This is for evaluation only\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "class CustomWikipediaRetriever:\n",
    "    def __init__(self, max_m=10, embedding_model=None):\n",
    "        self.retriever = WikipediaRetriever(top_k_results=max_m)\n",
    "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
    "        self.docs = []\n",
    "        self.similarities = []\n",
    "\n",
    "    def softmax(self, temperature, similarities):\n",
    "        \"\"\"Apply softmax with temperature to convert similarities into probabilities.\"\"\"\n",
    "        e_similarities = np.exp(np.array(similarities) / temperature)\n",
    "        return e_similarities / np.sum(e_similarities)\n",
    "\n",
    "    def add_noise_to_probabilities(self, probabilities, std_error):\n",
    "        \"\"\"Add normal distributed noise to the probabilities.\"\"\"\n",
    "        noise = np.random.normal(0, std_error, len(probabilities))\n",
    "        noisy_probabilities = probabilities + noise\n",
    "        \n",
    "        # Ensure all probabilities are non-negative\n",
    "        noisy_probabilities = np.maximum(noisy_probabilities, 0)\n",
    "        \n",
    "        # Normalize the probabilities so they sum to 1\n",
    "        return noisy_probabilities / np.sum(noisy_probabilities)\n",
    "\n",
    "    def retrieve_with_similarity(self, query):\n",
    "        # Get top-m documents from the retriever\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # Calculate the similarity scores\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        doc_embeddings = [self.embedding_model.embed_documents([doc.page_content])[0] for doc in docs]\n",
    "        \n",
    "        # Calculate cosine similarity scores\n",
    "        similarities = [1 - cosine(query_embedding, doc_emb) for doc_emb in doc_embeddings]\n",
    "        \n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        sorted_docs = [docs[i] for i in sorted_indices]\n",
    "        sorted_similarities = np.array(similarities)[sorted_indices]\n",
    "        \n",
    "        self.docs = sorted_docs\n",
    "        self.similarities = sorted_similarities\n",
    "\n",
    "    def retrieve(self, temperature=1.0, std_error=0.0, k=3, top_m=10):\n",
    "        # Convert similarities to probabilities\n",
    "\n",
    "        similarities = self.similarities[:min(len(self.docs),top_m)]\n",
    "        \n",
    "        probabilities = self.softmax(temperature, similarities)\n",
    "        \n",
    "        noisy_probabilities = self.add_noise_to_probabilities(probabilities, std_error)\n",
    "\n",
    "        # Randomly sample k documents from the top-m using probabilities\n",
    "        sampled_indices = np.random.choice(min(len(self.docs),top_m), size=k, replace=False, p=noisy_probabilities)\n",
    "        results = [self.docs[i] for i in sampled_indices]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, docs):\n",
    "        \"\"\"\n",
    "        Initialize the QAChain class with the desired number of retrieved documents (k).\n",
    "\n",
    "        Parameters:\n",
    "        - k: Number of documents to retrieve from Wikipedia for context (default is 3).\n",
    "        - embedding_model: The embedding model to use (default is OpenAIEmbeddings).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.docs = docs\n",
    "\n",
    "        # Define the prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Answer the question based only on the context provided as short as possible.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "        # Create the full chain\n",
    "        self.chain = (\n",
    "            {\"context\": self.retrieve_docs,  # Fixed method reference\n",
    "             \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def retrieve_docs(self, query):\n",
    "        \"\"\"\n",
    "        Retrieve and format documents from Wikipedia for the chain.\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"\\n\\n\".join(doc.page_content for doc in self.docs)\n",
    "    \n",
    "    def answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Answer a given question based on the context retrieved from Wikipedia.\n",
    "\n",
    "        Parameters:\n",
    "        - question: The question to ask.\n",
    "        \"\"\"\n",
    "        return self.chain.invoke(question)\n",
    "\n",
    "\n",
    "retriever = CustomWikipediaRetriever()\n",
    "query = \"who is the founder of quantum physics?\"\n",
    "retriever.retrieve_with_similarity(query)\n",
    "\n",
    "for i in range(3):\n",
    "    docs = retriever.retrieve()\n",
    "    print(docs[0].page_content[:100])\n",
    "    qa_chain = QAChain(docs=docs)\n",
    "    answer = qa_chain.answer(query)\n",
    "    print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744b612e-fb87-4895-8640-a991854450c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test = [5, 8, 12, 16, 20, 25]\n",
    "k_test = [1, 2, 3, 4, 5, 6]\n",
    "t_test = [0.1, 0.5, 1.0, 2.0, 10.0]\n",
    "e_test = [0.05, 0.1, 0.15, 0.2]\n",
    "num_sample = 500\n",
    "num_times = 3\n",
    "rng = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a644539-4817-4673-bcff-dd50392d729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = EvaluationMetrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c9efe7-5abd-4beb-a1f7-918ff37fc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataset import get_nq, get_tqa, get_squad, get_asqa\n",
    "\n",
    "nq = get_nq()\n",
    "tqa = get_tqa()\n",
    "squad = get_squad()\n",
    "asqa = get_asqa()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af314c5-9484-4056-9856-ab59bba32471",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_ds = {\"NQ\": nq, \"TriviaQA\": tqa, \"SQuAD\": squad, \"ASQA\": asqa}\n",
    "\n",
    "def evaluate(name):\n",
    "    ds = name_to_ds[name]\n",
    "    os.makedirs(\"top-m_results/vary_m\", exist_ok=True)\n",
    "    os.makedirs(\"top-m_results/vary_k\", exist_ok=True)\n",
    "    os.makedirs(\"top-m_results/temperature\", exist_ok=True)\n",
    "    os.makedirs(\"top-m_results/std_error\", exist_ok=True)\n",
    "\n",
    "    \n",
    "    random.seed(rng)\n",
    "    selected_idx = random.sample(range(ds.shape[0]), num_sample)\n",
    "\n",
    "    max_m = max(m_test)\n",
    "\n",
    "    references = ds.loc[selected_idx, \"answer\"].values.tolist()\n",
    "    \n",
    "    m_candidates = [[] for _ in range(len(m_test))]\n",
    "    m_results = []\n",
    "    k_candidates = [[] for _ in range(len(k_test))]\n",
    "    k_results = []\n",
    "    t_candidates = [[] for _ in range(len(t_test))]\n",
    "    t_results = []\n",
    "    e_candidates = [[] for _ in range(len(e_test))]\n",
    "    e_results = []\n",
    "    \n",
    "    for i in selected_idx:\n",
    "        q = ds.loc[i, \"question\"]\n",
    "        a = ds.loc[i, \"answer\"]\n",
    "        retriever = CustomWikipediaRetriever(max_m=max_m)\n",
    "        retriever.retrieve_with_similarity(q)\n",
    "        \n",
    "        for j in range(len(m_test)):\n",
    "            top_m = m_test[j]\n",
    "            c = []\n",
    "            for _ in range(num_times):\n",
    "                docs = retriever.retrieve(top_m=top_m)\n",
    "                qa_chain = QAChain(docs=docs)\n",
    "                answer = qa_chain.answer(q)\n",
    "                c.append(answer)\n",
    "                time.sleep(0.03)\n",
    "                m_results.append({\"idx in original dataset\":i, \"question\":q , \"top-m\": top_m, \"num_times\":_, \"retrieved docs\": docs, \"generated answer\": answer, \"standard answer\": a})\n",
    "            m_candidates[j].append(c)\n",
    "            \n",
    "        for j in range(len(k_test)):\n",
    "            k = k_test[j]\n",
    "            c = []\n",
    "            for _ in range(num_times):\n",
    "                docs = retriever.retrieve(k=k)\n",
    "                qa_chain = QAChain(docs=docs)\n",
    "                answer = qa_chain.answer(q)\n",
    "                c.append(answer)\n",
    "                time.sleep(0.03)\n",
    "                k_results.append({\"idx in original dataset\":i, \"question\":q , \"k\": k, \"num_times\":_, \"retrieved docs\": docs, \"generated answer\": answer, \"standard answer\": a})\n",
    "            k_candidates[j].append(c)\n",
    "            \n",
    "        for j in range(len(t_test)):\n",
    "            t = t_test[j]\n",
    "            c = []\n",
    "            for _ in range(num_times):\n",
    "                docs = retriever.retrieve(temperature=t)\n",
    "                qa_chain = QAChain(docs=docs)\n",
    "                answer = qa_chain.answer(q)\n",
    "                c.append(answer)\n",
    "                time.sleep(0.03)\n",
    "                t_results.append({\"idx in original dataset\":i, \"question\":q , \"temperature\": t, \"num_times\":_, \"retrieved docs\": docs, \"generated answer\": answer, \"standard answer\": a})\n",
    "            t_candidates[j].append(c)\n",
    "            \n",
    "        for j in range(len(e_test)):\n",
    "            e = e_test[j]\n",
    "            c = []\n",
    "            for _ in range(num_times):\n",
    "                docs = retriever.retrieve(std_error=e)\n",
    "                qa_chain = QAChain(docs=docs)\n",
    "                answer = qa_chain.answer(q)\n",
    "                c.append(answer)\n",
    "                time.sleep(0.03)\n",
    "                e_results.append({\"idx in original dataset\":i, \"question\":q , \"standard error\": e, \"num_times\":_, \"retrieved docs\": docs, \"generated answer\": answer, \"standard answer\": a})\n",
    "            e_candidates[j].append(c)\n",
    "\n",
    "    m_results = pd.DataFrame(m_results)\n",
    "    m_results.to_csv(f'top-m_results/vary_m/{name}.csv')\n",
    "    \n",
    "    k_results = pd.DataFrame(k_results)\n",
    "    k_results.to_csv(f'top-m_results/vary_k/{name}.csv')\n",
    "    \n",
    "    t_results = pd.DataFrame(t_results)\n",
    "    t_results.to_csv(f'top-m_results/temperature/{name}.csv')\n",
    "    \n",
    "    e_results = pd.DataFrame(e_results)\n",
    "    e_results.to_csv(f'top-m_results/std_error/{name}.csv')\n",
    "    \n",
    "    \n",
    "    m_rougeL, m_diversity = eval.cal_scores(m_candidates, references)\n",
    "    with open(f'top-m_results/vary_m/{name}.txt', 'w') as file:\n",
    "        sys.stdout = file\n",
    "        print(\"The top-m docs m are:\", m_test)\n",
    "        print(\"rougeL score for different m is:\", m_rougeL)\n",
    "        print(\"diversity score for different m is:\", m_diversity)\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "    \n",
    "    k_rougeL, k_diversity = eval.cal_scores(k_candidates, references) \n",
    "    with open(f'top-m_results/vary_k/{name}.txt', 'w') as file:\n",
    "        sys.stdout = file\n",
    "        print(\"The sampled k docs k are:\", k_test)\n",
    "        print(\"rougeL score for different k is:\", k_rougeL)\n",
    "        print(\"diversity score for different k is:\", k_diversity)\n",
    "    sys.stdout = sys.__stdout__\n",
    "    \n",
    "    \n",
    "    t_rougeL, t_diversity = eval.cal_scores(t_candidates, references)\n",
    "    with open(f'top-m_results/temperature/{name}.txt', 'w') as file:\n",
    "        sys.stdout = file\n",
    "        print(\"The temperature t are:\", t_test)\n",
    "        print(\"rougeL score for different t is:\", t_rougeL)\n",
    "        print(\"diversity score for different t is:\", t_diversity)\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "    \n",
    "    e_rougeL, e_diversity = eval.cal_scores(e_candidates, references)\n",
    "    with open(f'top-m_results/std_error/{name}.txt', 'w') as file:\n",
    "        sys.stdout = file\n",
    "        print(\"The standard error e are:\", e_test)\n",
    "        print(\"rougeL score for different e is:\", e_rougeL)\n",
    "        print(\"diversity score for different e is:\", e_diversity)\n",
    "    sys.stdout = sys.__stdout__\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436065fc-3555-459e-951b-b084f81b7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"NQ\", \"TriviaQA\", \"SQuAD\", \"ASQA\"]\n",
    "\n",
    "for ds in datasets:\n",
    "    evaluate(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f5d98-42fb-4e68-846e-f8364b56359b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
